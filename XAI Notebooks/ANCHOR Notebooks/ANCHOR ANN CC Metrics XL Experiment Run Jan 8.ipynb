{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6a486b-7a72-4691-8f57-260d79c5b234",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Disseration Experiment 5h\n",
    "# Generate ANCHOR Output (Credit Default) January EightÂ¶\n",
    "Ciaran Finnegan January 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326467c2-f2e4-41ef-8448-5175667109fd",
   "metadata": {},
   "source": [
    "# Import Libraries + Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03b5aa-a7a9-41a1-bb6a-56644cc198d8",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8884cf21-2d0d-457b-b738-8a9280059051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 19:18:54.622269: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-08 19:18:54.624831: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-08 19:18:54.673530: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-08 19:18:54.674922: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-08 19:18:55.591972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'anchor_tabular' from 'anchor' (/opt/conda/lib/python3.8/site-packages/anchor/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_199546/1350972530.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malibi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnchorTabular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0manchor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manchor_tabular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Libraries required for metrics calculations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'anchor_tabular' from 'anchor' (/opt/conda/lib/python3.8/site-packages/anchor/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Display libraries\n",
    "from IPython.display import display, HTML\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Import necessary libraries for ANN model building\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Import necessary library for ANCHOR explainer\n",
    "import alibi\n",
    "from alibi.explainers import AnchorTabular\n",
    "import anchor\n",
    "from anchor import anchor_tabular\n",
    "\n",
    "# Libraries required for metrics calculations\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "\n",
    "# Compute additional evaluation metrics\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Classifier training (not used for explainability)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Additional display libraires\n",
    "import contextlib\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Libraries used in Experiment Creation of XL Output Metrics\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cdb61-08f0-4ff4-88b4-4b7c8e58f67f",
   "metadata": {},
   "source": [
    "## Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4aea3-a604-4dac-955d-36d1e2898297",
   "metadata": {},
   "source": [
    "Dataset Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa82e3-eea0-4937-8596-f60dc62fb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./DS_Visualisation_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a56c4-5a07-4363-be32-2636409d57d2",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eee71a-d183-45d4-a9c6-113a1b6be4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./XAI_Metrics_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd556264-212b-4fd5-9231-9315bb5aaa82",
   "metadata": {},
   "source": [
    "Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17372074-9e8a-45c0-a176-50ecffaa7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./DS_Model_Build_Evaluation_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0dd94-f3cb-4d99-909b-d43bb79bc5fa",
   "metadata": {},
   "source": [
    "Track Experiment Result Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87bd84-e352-4c62-8c92-faccaa452ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./XAI_Experiment_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3919da-8422-4fc0-af8f-244466926beb",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f5037-d2de-467b-a6c9-f349aa02ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file_to_load = 'credit_default_data.csv'\n",
    "df = pd.read_csv(ds_file_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299a2c1-af6f-44aa-a71e-bac546b32f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the threshold for missing values\n",
    "threshold = 0.75 * len(df)\n",
    "\n",
    "# Identify columns with missing values greater than the threshold\n",
    "missing_columns = df.columns[df.isnull().sum() > threshold]\n",
    "\n",
    "# Print the columns with more than 75% missing values\n",
    "print(\"Columns with more than 75% missing values:\", missing_columns)\n",
    "\n",
    "# Drop columns with missing values greater than the threshold\n",
    "df = df.drop(columns=missing_columns)\n",
    "\n",
    "# Save or continue processing with columns removed that had high volumes of missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86654757-2644-4c90-aa82-7c3a8e192bab",
   "metadata": {},
   "source": [
    "## Categorical Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218545b-cfc6-4801-a6bd-890d8183d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns\n",
    "cat_cols = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71ce08-645a-4728-981a-14def4345050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa59ad-d33e-433a-aab3-ea3b7f26e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target variable\n",
    "target_distribution = df_encoded['default'].value_counts()\n",
    "\n",
    "target_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b153cd-d751-42ea-a9b4-fcabb15f6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the majority and minority classes\n",
    "df_majority = df_encoded[df_encoded['default'] == 0]\n",
    "df_minority = df_encoded[df_encoded['default'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce058168-bf75-4069-b3ad-e30624bf8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False, \n",
    "                                   n_samples=target_distribution[1], \n",
    "                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162cbf28-d3c1-42ea-8ff3-76ad732a616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the downsampled majority class with the minority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4ac2a-924e-446a-aed9-81f73f9ded0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset to mix the data points\n",
    "df_downsampled = df_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913bbf66-8c2a-40ca-b57c-bee98333dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the distribution of the target variable in the downsampled dataset\n",
    "df_downsampled['default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad4d198-0c4d-40b2-b576-a1deefa708ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the features and target variable\n",
    "X = df_downsampled.drop('default', axis=1)\n",
    "y = df_downsampled['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a5d2c-37ad-4242-87bf-93a7cf1c8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train_downsampled, X_test_downsampled, y_train_downsampled, y_test_downsampled = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58d2df-a65b-4174-ac51-7d18f9d5592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Indexes\n",
    "X_train_downsampled = X_train_downsampled.reset_index(drop=True)\n",
    "X_test_downsampled = X_test_downsampled.reset_index(drop=True)\n",
    "\n",
    "y_train_downsampled = y_train_downsampled.reset_index(drop=True)\n",
    "y_test_downsampled = y_test_downsampled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d613136f-ec24-4a28-b501-83cb0a1a27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_downsampled, X_test_downsampled = scale_the_features(X_train_downsampled, X_test_downsampled, df_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018ca08-fb2f-4f44-8fc9-b3ae578b48f5",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407376f6-c1f3-4d97-bb63-a3346a1b31fa",
   "metadata": {},
   "source": [
    "## Set Up Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc6153-a8a2-4d83-bcac-f9a9f40e4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53175934-1164-4f6c-85d1-278547bf3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_downsampled.shape[1],)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17b306-c7d8-4aa9-9ca9-b47d9bf8ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10cb6f-9e35-484c-97d8-5e30c73f463b",
   "metadata": {},
   "source": [
    "## Build Neural Network (w/TensorFlow/Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2f40d-3153-45b7-af92-65a62cf30d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "history = model.fit(X_train_downsampled, y_train_downsampled, epochs=15, batch_size=32, validation_split=0.2, verbose=1)\n",
    "print(\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e8334-6afb-43d7-9f06-ef6eb7197751",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e299520-a7a5-4e59-bc73-c53663190f4e",
   "metadata": {},
   "source": [
    "A Neural Network Model has been created in another Kubeflow Notebook and is being used in all the XAI experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875efdc-fe77-43c2-a29e-06f85ed1327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('ccfraud_model')  # If saved as SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5647a17-cb58-4ae8-a9cc-4fc9c02aa7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_loaded, y_test_loaded, X_train_loaded, y_train_loaded, df_downsampled_loaded, dfCatCols = load_CC_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25966a0d-83c7-4fe2-8692-45a6da32008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_loaded.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1377e11-b7a8-4ed1-931d-e79ff57e5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_loaded.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179ce5e-430d-44bc-ab4d-5fd8f50365ba",
   "metadata": {},
   "source": [
    "## Re-Display Model Peformance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39605a-04c0-4918-baae-77fbbdd4fb49",
   "metadata": {},
   "source": [
    "For illustration, the evualtion metrics of the NN model will be repeated here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f5bf2-aa60-419d-8799-69dc3dd67864",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbd31d-3e76-4e12-907b-6c1406625572",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()   \n",
    "X_test_loaded_scaled = scaler.fit_transform(X_test_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df03e60-ae68-4980-8b54-af4217e87058",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_loaded = display_model_metrics_tabular(loaded_model, X_test_loaded_scaled, y_test_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64028732-147f-422e-a3c2-9e7cad1c9d64",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6735005-c227-455f-ae8b-feebb5448293",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(y_test_loaded, y_pred_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210ce94-c311-4c4e-bc59-371969b2db43",
   "metadata": {},
   "source": [
    "## Assess and Display Model Peformance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09520c25-ccde-46c5-b1bb-a88a4a78c9a8",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abccc55-dda6-4166-8be0-9ec17ae3c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_funct = display_model_metrics_tabular(model, X_test_downsampled, y_test_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d465bc-1ce5-4f44-95b4-2c496cc97163",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5856f5f-e157-4b7a-a3e0-a0907fe381cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(y_test_downsampled, y_pred_funct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119542a4-b5a9-44d8-8f61-296092e9ef6b",
   "metadata": {},
   "source": [
    "# Generate ANCHOR Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a706c-aa38-4dc0-b31f-c685305d13fe",
   "metadata": {},
   "source": [
    "#### Suppress Warnings to clean up output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39feb23-23a9-41a8-b304-2c37bd766ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c813972-0c7d-4b99-84b6-1643ac0c1633",
   "metadata": {},
   "source": [
    "Check layout of X_train_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34511222-42f3-4cf5-a7e2-e2f4d82b3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0e0ff-0b41-432e-981b-4e4930008bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature names, excluding the target variable 'default'\n",
    "column_names = df_downsampled.drop('default', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06525ee1-337b-45a0-b9cd-03e435437997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy array to DataFrame\n",
    "X_train_downsampled = pd.DataFrame(X_train_downsampled, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45770351-5e28-440d-91b8-a6909b8cf66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_downsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85a485-037b-42c5-8931-a688baff372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor import anchor_tabular\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858f7eb-0f13-4692-9eb0-a11fae206035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features and the target variable\n",
    "X = df_encoded.drop('default', axis=1)\n",
    "y = df_encoded['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e237762-9db2-4fec-8b55-b75dd1b2d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the explainer\n",
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    class_names=['Not Default', 'Default'],\n",
    "    feature_names=X.columns.tolist(),\n",
    "    train_data=X_train_downsampled.values,\n",
    "    categorical_names={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8144f9-b6da-4757-80dc-7413f8815474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada52f7-9071-4bae-bcc5-74e9b877c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, 'w') as fnull:\n",
    "        with contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
    "            yield None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ac202-9c5c-4da3-b943-01d34cfbba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(x):\n",
    "    # Ensure x is in batch format\n",
    "    if len(x.shape) == 1:\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "    # Suppress the output of the progress bar\n",
    "    with suppress_stdout():\n",
    "        # Get the model's prediction (probability of the positive class)\n",
    "        probabilities = model.predict(x, verbose=0)\n",
    "    # Convert probabilities to class labels (0 or 1)\n",
    "    labels = (probabilities > 0.5).astype(int)\n",
    "    return labels.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5abc32-c320-4c13-950d-9a2297f85681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy array to DataFrame\n",
    "X_test_downsampled = pd.DataFrame(X_test_downsampled, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3042541-69c9-47b2-ab15-6c89a731e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the instance passed to explain_instance is in the correct shape\n",
    "idx = 0\n",
    "instance_to_explain = X_test_downsampled.iloc[idx].values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296a832-e658-4ecf-bebd-617ac1ebae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an explanation for the first instance in the test set\n",
    "exp = explainer.explain_instance(instance_to_explain, predict_fn, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c65572-e59b-4ad5-bd4a-1323e73b2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the explanation\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37431d6-bfa9-434e-8042-db7cafe4b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec71b11c-669d-474f-92e9-e3432f40c6ef",
   "metadata": {},
   "source": [
    "#### Pseudocode to Generate Initial ANCHOR Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8067dd-5f76-4a3d-8b29-41dfd7a2d348",
   "metadata": {},
   "source": [
    "For the RF model built above in Python, select a random sample \n",
    "of 15 instances in the test data, 10 for Class '0' and 5 for \n",
    "Class '1', and generate ANCHOR values as explainers for these  \n",
    "instances in the test dataset.\n",
    "\n",
    "Present these ANCHOR values in an easily understood and pleasant \n",
    "on the eye tabular output format for the Python Kubeflow Notebook\n",
    "in which I am writing my Python code. \n",
    "\n",
    "Create a second tabular format what shows an equally appealing \n",
    "output in my Python Notebook that shows the ANCHOR values and the\n",
    "feature details for each instance on a single row, across which I\n",
    "can scroll.\n",
    "\n",
    "Comment each line of Python code with as much detail as practical. \n",
    "\n",
    "Output the ANCHOR values to a CSV file. Output the feature details \n",
    "for each corresponding instance for which the ANCHOR Values were\n",
    "created in a seperate CSV file.\n",
    "\n",
    "After the code generation provide as much narrative detail \n",
    "as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262310f-4581-4824-a5d9-bd1115485ec9",
   "metadata": {},
   "source": [
    "##### Further pseudocode..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e4abb-a81c-4ba1-9a1e-65f4a3707713",
   "metadata": {},
   "source": [
    "Use the AnchorTabular explainer from the alibi library. This explainer provides local explanations for classification models' predictions by identifying a minimal set of conditions (features) in the instance that ensure the model's decision remains unchanged (these conditions are called \"anchors\").\n",
    "\n",
    "The steps:\n",
    "\n",
    "Select a random sample of 15 instances from the test data, 10 from Class '0' and 5 from Class '1'.\n",
    "Set up the AnchorTabular explainer and fit it to the training data.\n",
    "Generate anchor explanations for the selected instances.\n",
    "Present the anchor values in two tabular formats: a summary table and a detailed table.\n",
    "Output the anchor values and feature details to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f957338-e9a7-4286-80c2-b520879ee036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the first five instances in the test dataset\n",
    "for idx in range(5):\n",
    "    instance = X_test_downsampled.iloc[idx].values.reshape(1, -1)\n",
    "    print(f\"\\nInstance {idx + 1}:\")\n",
    "    \n",
    "    # Generate an explanation for the instance\n",
    "    exp = explainer.explain_instance(instance, predict_fn, threshold=0.95)\n",
    "    \n",
    "    # Show the explanation in the notebook\n",
    "    exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f9249-0375-46c9-842b-afe2c335123a",
   "metadata": {},
   "source": [
    "### Create an ANCHOR File Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908eed01-2b74-4f26-bbe8-39c9b56ae579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the ANCHOR results\n",
    "anchor_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ced75-3da1-4fb6-a4ea-9dcf2786859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "anchor_results = []\n",
    "\n",
    "# Loop through the first five instances in the test dataset\n",
    "for idx in range(5):\n",
    "    instance = X_test_downsampled.iloc[idx].values.reshape(1, -1)\n",
    "    # Generate an explanation for the instance\n",
    "    exp = explainer.explain_instance(instance, predict_fn, threshold=0.95)\n",
    "    \n",
    "    # Extract feature importance from the explanation\n",
    "    feature_importance = {}\n",
    "    for condition in exp.names():\n",
    "        # Handle conditions with '='\n",
    "        if '=' in condition:\n",
    "            feature, value = condition.split('=')\n",
    "            feature = feature.strip()\n",
    "            value = float(value.strip())\n",
    "            feature_importance[feature] = ('=', value)\n",
    "        # Handle conditions with '>' or '<'\n",
    "        elif '>' in condition or '<' in condition:\n",
    "            parts = re.split('([><])', condition)\n",
    "            feature, operator, value = [part.strip() for part in parts if part.strip()]\n",
    "            value = float(value)\n",
    "            feature_importance[feature] = (operator, value)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected format for ANCHOR explanation: {condition}\")\n",
    "\n",
    "    anchor_results.append(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23140bb-b6cb-45a9-92c7-ed2f15233a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the results\n",
    "df_anchor_results = pd.DataFrame(anchor_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419ce66-5465-4601-9eef-9555c4129572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrame\n",
    "print(df_anchor_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75987bd-cb41-4576-a5d1-1b849b63978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "df_anchor_results.to_csv('anchor_results_ANN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35439af3-1c0f-4c6a-a759-89124fd22e4d",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016d008-5f44-4e2f-8c16-34c868372354",
   "metadata": {},
   "source": [
    "# Prepare ANCHOR Values for Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58773ba7-9ff6-4231-8b30-e985f99da9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def generate_anchors_for_instances(df, num_instances=5):\n",
    "    # Initialize a list to store the ANCHOR results\n",
    "    new_anchor_results = []\n",
    "    feature_instances = []\n",
    "\n",
    "    # Loop through the first five instances in the test dataset\n",
    "    for idx in range(num_instances):\n",
    "        #instance = X_test_downsampled.iloc[idx]\n",
    "        instance = df.iloc[idx]\n",
    "        feature_instances.append(instance)\n",
    "\n",
    "        # Generate an explanation for the instance with a lower threshold\n",
    "        exp = explainer.explain_instance(instance.values.reshape(1, -1), predict_fn, threshold=0.99)\n",
    "\n",
    "        # Check if an explanation was found\n",
    "        if exp is not None:\n",
    "            # Parse the conditions from the explanation and format them\n",
    "            anchor_explanation = []\n",
    "            for condition in exp.names():\n",
    "                if ' > ' in condition or ' < ' in condition:\n",
    "                    feature, relation, value = condition.split(' ')[0], condition.split(' ')[1], condition.split(' ')[2]\n",
    "                    try:\n",
    "                        anchor_explanation.append(f\"'{feature} {relation} {float(value):.2f}'\")\n",
    "                    except ValueError:\n",
    "                        anchor_explanation.append(f\"'{condition}'\")\n",
    "                else:\n",
    "                    anchor_explanation.append(f\"'{condition}'\")\n",
    "\n",
    "            # Convert the list of strings to a single string\n",
    "            anchor_explanation_str = '[' + ', '.join(anchor_explanation) + ']'\n",
    "\n",
    "            # Add the formatted explanation to the results list\n",
    "            new_anchor_results.append(anchor_explanation_str)\n",
    "        else:\n",
    "            new_anchor_results.append(\"['No explanation found']\")\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    new_df_anchor_results = pd.DataFrame(new_anchor_results, columns=['Anchor Explanation'])\n",
    "\n",
    "    # Create a DataFrame from the feature instances\n",
    "    df_feature_instances = pd.DataFrame(feature_instances)\n",
    "    \n",
    "    \n",
    "    return df_feature_instances, new_df_anchor_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3948cf4-ae94-4525-bde8-a92c7bcc0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_feature_instances, new_df_anchor_results = generate_anchors_for_instances(X_test_downsampled, 5)\n",
    "results, exec_time = generate_anchors_for_instances(X_test_downsampled, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac31c512-9579-4066-bf50-18b90a61c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_instances, new_df_anchor_results = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f1944-8f9d-474f-ae0a-2694119199b6",
   "metadata": {},
   "source": [
    "## Determine Computational Efficiency Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d51de-b10b-4021-8477-f5c785549499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display time to generate DiCE explainers\n",
    "print(f\"ANCHORS Execution Time: {exec_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906265d8-c510-404e-b04b-423946e42943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrames\n",
    "print(\"Anchor Explanations:\")\n",
    "print(new_df_anchor_results)\n",
    "#print(\"\\nFeature Instances:\")\n",
    "#print(df_feature_instances)\n",
    "\n",
    "# Write the DataFrames to CSV files\n",
    "new_df_anchor_results.to_csv('new_anchor_results5.csv', index=False)\n",
    "df_feature_instances.to_csv('feature_instances5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81355e6b-81a0-47de-badb-875d6e235551",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_instances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da878039-4fe3-497a-8428-454144d2b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_anchor_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae91cf1-3078-4538-a8ad-caf40f177f61",
   "metadata": {},
   "source": [
    "## Parse the Anchor Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdecc84e-401e-46b7-879b-4761dab8ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "# Step 2: Convert the 'Anchor Explanation' column from a string representation of a list back to an actual list\n",
    "new_df_anchor_results['Anchor Explanation'] = new_df_anchor_results['Anchor Explanation'].apply(ast.literal_eval)\n",
    "\n",
    "# Step 3: Determine the maximum number of conditions in the ANCHOR explanations across all instances\n",
    "max_num_conditions = max(new_df_anchor_results['Anchor Explanation'].apply(len))\n",
    "\n",
    "# Step 4: Initialize a list to store the numerical representations of the ANCHOR explanations\n",
    "numerical_explanations = []\n",
    "\n",
    "# Step 5: Loop through each ANCHOR explanation and convert it to a numerical representation\n",
    "for explanation in new_df_anchor_results['Anchor Explanation']:\n",
    "    numerical_representation = [-1] * len(df_feature_instances.columns) * max_num_conditions\n",
    "    for idx, condition in enumerate(explanation):\n",
    "        # Parse the condition to extract the feature name and value\n",
    "        feature, relation, value = condition.split(' ')[0], condition.split(' ')[1], condition.split(' ')[2]\n",
    "        \n",
    "        # Find the index of the feature in the feature dataframe\n",
    "        feature_idx = df_feature_instances.columns.get_loc(feature)\n",
    "        \n",
    "        # Store the feature index in the numerical representation\n",
    "        numerical_representation[feature_idx * max_num_conditions + idx] = float(value)\n",
    "    numerical_explanations.append(numerical_representation)\n",
    "\n",
    "# Step 6: Create a dataframe from the numerical representations\n",
    "df_anchors_numerical = pd.DataFrame(numerical_explanations)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(df_anchors_numerical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6ccf1-fcdb-4ab6-a9b6-8d985a56b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_anchor_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b3603-1b51-44fb-bec2-9649f2b2c8a2",
   "metadata": {},
   "source": [
    "## Display ANCHORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26960e2-6520-4a42-8e8d-6924aa02182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of each dataset to understand their structure\n",
    "instance_features_head = df_feature_instances.head()\n",
    "anchor_explanations_head = new_df_anchor_results.head()\n",
    "anchor_explanations_numerical = df_anchors_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c1ed8-959c-4a92-a2f9-b6b670be4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_features_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82eafe-2889-4097-8a0e-09a5c9ecdc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_explanations_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bd68c-ca9e-41cb-ae8a-89c699dbda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_explanations_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4656447d-1e45-4aee-923e-fea4e1cf58d1",
   "metadata": {},
   "source": [
    "## Generate Outfile for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f2617-a30a-4afd-adf7-cc50712be650",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_num_explainers_filepath = \"anchor_numerical_explainers.csv\"\n",
    "\n",
    "anchor_explanations_numerical.to_csv(anchors_num_explainers_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d16df-2e95-47a7-828f-c1caa1c007ff",
   "metadata": {},
   "source": [
    "# Generate XAI Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1840f4dc-2390-47b9-a047-d8f225bfea9a",
   "metadata": {},
   "source": [
    "## Identity Metric "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd86065-15b6-4d1e-a3ff-5c5db705ed08",
   "metadata": {},
   "source": [
    "#### Run a Basic Test First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72312d8-3ee2-4df6-b213-67e1cf4ca814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two random instances from the ANCHOR dataframe\n",
    "df_xai_numerical = anchor_explanations_numerical\n",
    "\n",
    "random_indices = np.random.choice(df_xai_numerical.index, size=2, replace=False)\n",
    "instance_1 = df_xai_numerical.iloc[random_indices[0]]\n",
    "instance_2 = df_xai_numerical.iloc[random_indices[1]]\n",
    "\n",
    "# Compute the Euclidean distance between the selected instances - uses custom project function\n",
    "distance = get_euclidean_distance(instance_1, instance_2)\n",
    "print(f\"Euclidean distance between instance {random_indices[0]} and instance {random_indices[1]}: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c369f7d-a6fb-4506-b85a-f994361c2848",
   "metadata": {},
   "source": [
    "#### Retrieve Identity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a546d3f-53cb-41c5-b5ac-cfb441b7de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0eafbb-ff1d-4df0-ba35-8d8bd4725a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Identity_Metric = get_identity_metric(df_feature_instances, anchor_explanations_numerical, \"ANCHOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b0ada-5676-46de-913f-50bbe5b82a0f",
   "metadata": {},
   "source": [
    "#### Display Identity Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2ae16-ce65-4605-b51c-665a56d5510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Identity_Number = \"{:.2f}%\".format(ANCHOR_Identity_Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f342a-64fe-44ba-8bd1-522cfa80b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text(\"ANCHOR Identity Metric Score: \" + ANCHOR_Identity_Number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047864c4-2600-405b-b6db-9d1ae6d57db8",
   "metadata": {},
   "source": [
    "## Stability Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8dd97a-e8ff-4de2-9d27-0e4638db4030",
   "metadata": {},
   "source": [
    "### Invoke Stability Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a2935-fc4d-4485-92a4-90c2b9ead621",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Stability_Metric = get_stability_metric_y(anchor_explanations_numerical, y_test_downsampled, 'ANCHOR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf6776-0a63-4b6d-b5a5-33e848749914",
   "metadata": {},
   "source": [
    "#### Display Stability Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45e85f-faca-42e2-8efe-3db48a005ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Stability_Number = \"{:.2f}%\".format(ANCHOR_Stability_Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c9796-7091-45ce-a8ec-65dcb6bc52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text(\"ANCHOR Stability Metric Score: \" + ANCHOR_Stability_Number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc93a738-a9eb-4e99-aa3c-b04d4840b91c",
   "metadata": {},
   "source": [
    "## Seperability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020429dc-7ba7-4e30-8c65-74d87de2ef9e",
   "metadata": {},
   "source": [
    "### Invoke Seperability Metric Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b0b7b-506e-4031-a55f-daf2ddee0188",
   "metadata": {},
   "source": [
    "#### Retrieve Seperability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ee5df-706a-473d-969f-82b2163b1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Seperability_Metric = get_seperability_metric(df_feature_instances, anchor_explanations_numerical, \"ANCHOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2f687-4b34-4deb-ace4-f9d1289334a3",
   "metadata": {},
   "source": [
    "#### Display Seperability Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b47c2-ea12-411d-bfe9-9452db0e5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Seperability_Number = \"{:.2f}%\".format(ANCHOR_Seperability_Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4c271-5d06-401f-91a9-73dceb5cc860",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text(\"ANCHOR Seperability Metric Score: \" + ANCHOR_Seperability_Number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b373cb-3a90-4d3e-ae80-f2b5c0da541c",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614cb4a5-3d19-41e3-9133-7fc1ed85bec5",
   "metadata": {},
   "source": [
    "### Invoke Similarity Metric Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067f3f2-f277-4311-a03c-e20cf8ab2b03",
   "metadata": {},
   "source": [
    "#### Retrieve Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5cccf-414e-4a2b-8b57-753b791a485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Similarity_Metric = get_similarity_metric(df_feature_instances, anchor_explanations_numerical, \"ANCHOR\", \n",
    "                                                 use_dbscan=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf26d4a-feee-4369-9470-7e90e3ed0048",
   "metadata": {},
   "source": [
    "#### Display Similarity Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4c913-95c8-415b-a17a-81cd34905f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Similarity_Number = \"{:6.2f}\".format(ANCHOR_Similarity_Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd05474-eaa0-4fd3-bcad-8eafa3feda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text(\"ANCHOR Similarity Metric Value: \" + ANCHOR_Similarity_Number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
