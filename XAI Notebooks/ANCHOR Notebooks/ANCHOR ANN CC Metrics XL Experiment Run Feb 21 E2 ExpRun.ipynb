{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6a486b-7a72-4691-8f57-260d79c5b234",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Disseration Experiment 5k\n",
    "# Generate ANCHOR Output (Credit Default) February 21Â¶\n",
    "Ciaran Finnegan February 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326467c2-f2e4-41ef-8448-5175667109fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Libraries + Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03b5aa-a7a9-41a1-bb6a-56644cc198d8",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884cf21-2d0d-457b-b738-8a9280059051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Display libraries\n",
    "from IPython.display import display, HTML\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Import necessary libraries for ANN model building\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Import necessary library for ANCHOR explainer\n",
    "import alibi\n",
    "from alibi.explainers import AnchorTabular\n",
    "#import anchor\n",
    "from anchor import anchor_tabular\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Libraries required for metrics calculations\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "\n",
    "# Compute additional evaluation metrics\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Classifier training (not used for explainability)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Additional display libraires\n",
    "import contextlib\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Libraries used in Experiment Creation of XL Output Metrics\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cdb61-08f0-4ff4-88b4-4b7c8e58f67f",
   "metadata": {},
   "source": [
    "## Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4aea3-a604-4dac-955d-36d1e2898297",
   "metadata": {},
   "source": [
    "Dataset Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa82e3-eea0-4937-8596-f60dc62fb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./DS_Visualisation_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a56c4-5a07-4363-be32-2636409d57d2",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eee71a-d183-45d4-a9c6-113a1b6be4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./XAI_Metrics_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd556264-212b-4fd5-9231-9315bb5aaa82",
   "metadata": {},
   "source": [
    "Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17372074-9e8a-45c0-a176-50ecffaa7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./DS_Model_Build_Evaluation_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0dd94-f3cb-4d99-909b-d43bb79bc5fa",
   "metadata": {},
   "source": [
    "Track Experiment Result Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87bd84-e352-4c62-8c92-faccaa452ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./XAI_Experiment_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3919da-8422-4fc0-af8f-244466926beb",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e8334-6afb-43d7-9f06-ef6eb7197751",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e299520-a7a5-4e59-bc73-c53663190f4e",
   "metadata": {},
   "source": [
    "A Neural Network Model has been created in another Kubeflow Notebook and is being used in all the XAI experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875efdc-fe77-43c2-a29e-06f85ed1327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('ccfraud_model')  # If saved as SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5647a17-cb58-4ae8-a9cc-4fc9c02aa7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_loaded, y_test_loaded, X_train_loaded, y_train_loaded, df_downsampled_loaded, dfCatCols = load_CC_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25966a0d-83c7-4fe2-8692-45a6da32008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_loaded.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1377e11-b7a8-4ed1-931d-e79ff57e5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_loaded.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179ce5e-430d-44bc-ab4d-5fd8f50365ba",
   "metadata": {},
   "source": [
    "## Re-Display Model Peformance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39605a-04c0-4918-baae-77fbbdd4fb49",
   "metadata": {},
   "source": [
    "For illustration, the evualtion metrics of the NN model will be repeated here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f5bf2-aa60-419d-8799-69dc3dd67864",
   "metadata": {},
   "source": [
    "### Re-Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbd31d-3e76-4e12-907b-6c1406625572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale_loaded = StandardScaler()   \n",
    "#X_train_loaded_scaled = scale_loaded.fit_transform(X_train_loaded)\n",
    "#X_test_loaded_scaled  = scale_loaded.transform(X_test_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a059d1-a4d7-4614-bc3e-e697a251cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_loaded_scaled, X_test_loaded_scaled, scale_loaded = scale_the_features(X_train_loaded, \n",
    "                                                                                X_test_loaded, \n",
    "                                                                                df_downsampled_loaded, \n",
    "                                                                                'Fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc45636-b5de-4261-9fe3-f648dd38df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_loaded_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba408485-dd1b-4001-9db3-f74793ec3b98",
   "metadata": {},
   "source": [
    "### Re-evaluate loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df03e60-ae68-4980-8b54-af4217e87058",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_loaded = display_model_metrics_tabular(loaded_model, X_test_loaded_scaled, y_test_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64028732-147f-422e-a3c2-9e7cad1c9d64",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6735005-c227-455f-ae8b-feebb5448293",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(y_test_loaded, y_pred_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119542a4-b5a9-44d8-8f61-296092e9ef6b",
   "metadata": {},
   "source": [
    "# Generate ANCHOR Values (Examples Instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a706c-aa38-4dc0-b31f-c685305d13fe",
   "metadata": {},
   "source": [
    "#### Suppress Warnings to clean up output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39feb23-23a9-41a8-b304-2c37bd766ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db240e-c707-4352-b0ec-0fe7c59102de",
   "metadata": {},
   "source": [
    "#### Prepare Data Inputs to Anchor Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c813972-0c7d-4b99-84b6-1643ac0c1633",
   "metadata": {},
   "source": [
    "Check layout of X_train_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0e0ff-0b41-432e-981b-4e4930008bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature names, excluding the target variable 'default'\n",
    "# Jan 6th - use new model and data\n",
    "column_names = df_downsampled_loaded.drop('Fraud', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858f7eb-0f13-4692-9eb0-a11fae206035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features and the target variable\n",
    "# Jan 6th - use new model and data\n",
    "X = df_downsampled_loaded.drop('Fraud', axis=1)\n",
    "y = df_downsampled_loaded['Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38135d-1943-4347-a824-f397e70815d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy array to DataFrame\n",
    "X_train_loaded_scaled = pd.DataFrame(X_train_loaded_scaled, columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c9081f-89a9-4cf0-b7f7-d7bc769d5c87",
   "metadata": {},
   "source": [
    "#### Set Up Anchor Explainer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b650d74c-47fb-410a-acd7-49e2b3b93629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_loaded_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72171928-ea39-4879-aed5-90c974c21f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Anchor explainer\n",
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    \n",
    "    class_names=['Non Fraud', 'Fraud'],\n",
    "    \n",
    "    feature_names=X.columns.tolist(),\n",
    "    \n",
    "    # Jan 18th - use new loaded model and data - SCALED\n",
    "    train_data=X_train_loaded_scaled.values,\n",
    "    \n",
    "    categorical_names={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada52f7-9071-4bae-bcc5-74e9b877c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, 'w') as fnull:\n",
    "        with contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
    "            yield None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ac202-9c5c-4da3-b943-01d34cfbba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(x):\n",
    "    # Ensure x is in batch format\n",
    "    if len(x.shape) == 1:\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "    # Suppress the output of the progress bar\n",
    "    with suppress_stdout():\n",
    "        \n",
    "        # Get the model's prediction (probability of the positive class)\n",
    "        #probabilities = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Jan 6th - use new model and data\n",
    "        probabilities = loaded_model.predict(x, verbose=0)\n",
    "        \n",
    "    # Convert probabilities to class labels (0 or 1)\n",
    "    labels = (probabilities > 0.5).astype(int)\n",
    "    return labels.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d34e0-6ebd-4628-a243-6b360577401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188c860-25cc-4645-9b97-d94175b356f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_loaded_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02031e6-4432-4ef7-9ef5-6ed2534c51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy array to DataFrame\n",
    "X_test_loaded_scaled = pd.DataFrame(X_test_loaded_scaled, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3042541-69c9-47b2-ab15-6c89a731e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the instance passed to explain_instance is in the correct shape\n",
    "idx = 4\n",
    "\n",
    "# Select an instance from the test data with which to generate an Anchor explanation\n",
    "# instance_to_explain = X_test_loaded.iloc[idx].values.reshape(1, -1)\n",
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "instance_to_explain = X_test_loaded_scaled.iloc[idx].values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296a832-e658-4ecf-bebd-617ac1ebae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an explanation for the first instance in the test set\n",
    "exp = explainer.explain_instance(instance_to_explain, predict_fn, threshold=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8e8ba-8c43-47e0-aaad-960a38656109",
   "metadata": {},
   "source": [
    "#### Display Anchor Explainers (Single Instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c65572-e59b-4ad5-bd4a-1323e73b2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the explanation\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37431d6-bfa9-434e-8042-db7cafe4b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d29b4-e8da-44e8-9958-b49572a27c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unscaled_feature_values_orig(exp, scaler, feature_names):\n",
    "        \n",
    "    # Convert feature_names to a list if it's a pandas Index\n",
    "    if isinstance(feature_names, pd.Index):\n",
    "        feature_names = feature_names.tolist()\n",
    "    \n",
    "    \n",
    "    original_features_and_values = {}\n",
    "\n",
    "    for condition in exp.names():\n",
    "        # Check for the type of condition and split accordingly\n",
    "        if '<=' in condition:\n",
    "            left, right = condition.split('<=', 1)\n",
    "            operator = '<='\n",
    "        elif '>=' in condition:\n",
    "            left, right = condition.split('>=', 1)\n",
    "            operator = '>='\n",
    "        elif '<' in condition:\n",
    "            left, right = condition.split('<', 1)\n",
    "            operator = '<'\n",
    "        elif '>' in condition:\n",
    "            left, right = condition.split('>', 1)\n",
    "            operator = '>'\n",
    "        else:\n",
    "            continue  # Skip if the condition format is not recognized\n",
    "\n",
    "        feature = left.strip()\n",
    "        scaled_value = float(right.strip())\n",
    "\n",
    "        # Find the index of the feature in the original dataset\n",
    "        feature_index = feature_names.index(feature)\n",
    "\n",
    "        # Create a dummy array for inverse transformation\n",
    "        dummy_array = np.zeros((1, len(feature_names)))\n",
    "        dummy_array[0, feature_index] = scaled_value\n",
    "    \n",
    "\n",
    "        # Inverse transform to get the original value\n",
    "        original_value = scaler.inverse_transform(dummy_array)[0, feature_index]\n",
    "        \n",
    "\n",
    "        # Adjust the original value if it's very close to 0 or 1\n",
    "        if -0.01 <= original_value <= 0.02:\n",
    "            original_value = 0.00\n",
    "        elif 0.99 <= original_value <= 1.01:\n",
    "            original_value = 1.00\n",
    "\n",
    "        # Store the condition with the original value\n",
    "        original_features_and_values[feature + ' ' + operator] = original_value\n",
    "\n",
    "    return original_features_and_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111d96d-9f74-4312-8ec3-d61c866ecb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unscaled_feature_values(exp, scaler, feature_names, scaled_feature_names):\n",
    "    \n",
    "    # Convert feature_names to a list if it's a pandas Index\n",
    "    if isinstance(feature_names, pd.Index):\n",
    "        feature_names = feature_names.tolist()\n",
    "    \n",
    "    # Dictionary to store the original features and values\n",
    "    original_features_and_values = {}\n",
    "\n",
    "    # Iterate over conditions provided by the explainer\n",
    "    for condition in exp.names():\n",
    "        # Split the condition to extract the feature and its scaled value\n",
    "        if '<=' in condition:\n",
    "            left, right = condition.split('<=', 1)\n",
    "            operator = '<='\n",
    "        elif '>=' in condition:\n",
    "            left, right = condition.split('>=', 1)\n",
    "            operator = '>='\n",
    "        elif '<' in condition:\n",
    "            left, right = condition.split('<', 1)\n",
    "            operator = '<'\n",
    "        elif '>' in condition:\n",
    "            left, right = condition.split('>', 1)\n",
    "            operator = '>'\n",
    "        else:\n",
    "            continue  # Skip if the condition format is not recognized\n",
    "\n",
    "        feature = left.strip()\n",
    "        scaled_value = float(right.strip())\n",
    "\n",
    "        # Only proceed if the feature was scaled\n",
    "        if feature in scaled_feature_names:\n",
    "            # Find the index of the feature in the scaled features\n",
    "            feature_index_scaled = scaled_feature_names.index(feature)\n",
    "            \n",
    "            # Create a dummy array for inverse transformation\n",
    "            # This array should only include scaled (non-binary) features\n",
    "            dummy_array_scaled = np.zeros((1, len(scaled_feature_names)))\n",
    "            dummy_array_scaled[0, feature_index_scaled] = scaled_value\n",
    "\n",
    "            # Inverse transform to get the original value of the scaled feature\n",
    "            original_value_scaled = scaler.inverse_transform(dummy_array_scaled)[0, feature_index_scaled]\n",
    "        else:\n",
    "            # For binary features, the original value is the same as the scaled value\n",
    "            original_value_scaled = scaled_value\n",
    "\n",
    "        # Adjust the original value if it's very close to 0 or 1, this might be necessary for binary features\n",
    "        if -0.01 <= original_value_scaled <= 0.02:\n",
    "            original_value_scaled = 0.00\n",
    "        elif 0.99 <= original_value_scaled <= 1.01:\n",
    "            original_value_scaled = 1.00\n",
    "\n",
    "        # Store the condition with the original (or adjusted) value\n",
    "        original_features_and_values[feature + ' ' + operator] = original_value_scaled\n",
    "\n",
    "    return original_features_and_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2832d8-8eff-4662-a713-99a40a7d3285",
   "metadata": {},
   "source": [
    "To improve meaningfulness of ANCHOR explainer set up an inverse scale on the non-binary features, which were previously scaled in model/XAI building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5248d71-3279-4491-be22-8a0106f8e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-binary scaled features\n",
    "scaled_features = [col for col in X_test_loaded_scaled.columns if not X_test_loaded_scaled[col].dropna().isin([0, 1]).all()]\n",
    "\n",
    "print(\"\\n\\nAll CC Fraud Non-Binary features (NOT to be scaled):\", scaled_features)\n",
    "print(\"\\n\\n\") # Gap for formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ba1e8-82cb-4979-851a-a7db35993650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'exp' is the Anchor explanation, scaler is the scaler used, \n",
    "# and feature_names is a list of feature names\n",
    "original_features_and_values = get_unscaled_feature_values(exp, \n",
    "                                                           scale_loaded, \n",
    "                                                           column_names,\n",
    "                                                           scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e6164-77c3-45f8-b715-64714e9efc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_features_and_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343d0e6-eec2-49fa-a67e-22e805b79ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_unscaled_feature_values(original_features_and_values):\n",
    "    # Start with an empty string for HTML content\n",
    "    html_content = '<div style=\"font-family: Arial; padding: 10px; border: 1px solid #ddd; border-radius: 5px; background-color: #f9f9f9;\">'\n",
    "    html_content += '<h2 style=\"color: #4CAF50;\">Anchor Explanation (Unscaled Feature Values)</h2>'\n",
    "\n",
    "    # Iterate over the dictionary and add items to the HTML content\n",
    "    for feature_condition, value in original_features_and_values.items():\n",
    "        html_content += f'<p><b>{feature_condition}</b>: {value:.2f}</p>'\n",
    "\n",
    "    html_content += '</div>'\n",
    "    \n",
    "    # Display the HTML content\n",
    "    display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca295a-b6ce-4e71-8cf8-37a18feab2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the explanation\n",
    "exp.show_in_notebook()\n",
    "# Generate imnproved visuals for Anchor ouput for sample instance\n",
    "display_unscaled_feature_values(original_features_and_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff6f38-ccf0-4760-97f2-483ee0d95560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec71b11c-669d-474f-92e9-e3432f40c6ef",
   "metadata": {},
   "source": [
    "#### Pseudocode to Generate Initial ANCHOR Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8067dd-5f76-4a3d-8b29-41dfd7a2d348",
   "metadata": {},
   "source": [
    "For the RF model built above in Python, select a random sample \n",
    "of 15 instances in the test data, 10 for Class '0' and 5 for \n",
    "Class '1', and generate ANCHOR values as explainers for these  \n",
    "instances in the test dataset.\n",
    "\n",
    "Present these ANCHOR values in an easily understood and pleasant \n",
    "on the eye tabular output format for the Python Kubeflow Notebook\n",
    "in which I am writing my Python code. \n",
    "\n",
    "Create a second tabular format what shows an equally appealing \n",
    "output in my Python Notebook that shows the ANCHOR values and the\n",
    "feature details for each instance on a single row, across which I\n",
    "can scroll.\n",
    "\n",
    "Comment each line of Python code with as much detail as practical. \n",
    "\n",
    "Output the ANCHOR values to a CSV file. Output the feature details \n",
    "for each corresponding instance for which the ANCHOR Values were\n",
    "created in a seperate CSV file.\n",
    "\n",
    "After the code generation provide as much narrative detail \n",
    "as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262310f-4581-4824-a5d9-bd1115485ec9",
   "metadata": {},
   "source": [
    "Further pseudocode..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e4abb-a81c-4ba1-9a1e-65f4a3707713",
   "metadata": {},
   "source": [
    "Use the AnchorTabular explainer from the alibi library. This explainer provides local explanations for classification models' predictions by identifying a minimal set of conditions (features) in the instance that ensure the model's decision remains unchanged (these conditions are called \"anchors\").\n",
    "\n",
    "The steps:\n",
    "\n",
    "Select a random sample of 15 instances from the test data, 10 from Class '0' and 5 from Class '1'.\n",
    "Set up the AnchorTabular explainer and fit it to the training data.\n",
    "Generate anchor explanations for the selected instances.\n",
    "Present the anchor values in two tabular formats: a summary table and a detailed table.\n",
    "Output the anchor values and feature details to CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad12d55-56ee-481e-a03f-26d1cb74f4d0",
   "metadata": {},
   "source": [
    "#### Display Anchor Explainers (Multiple Instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a274bb-d3ba-48c4-b17f-ac6e459ea2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "#instance_to_explain = X_test_loaded_scaled.iloc[idx].values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f957338-e9a7-4286-80c2-b520879ee036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the first five instances in the test dataset\n",
    "for idx in range(7):\n",
    "       \n",
    "    # Jan 6th - use new model and data\n",
    "    #instance = X_test_loaded.iloc[idx].values.reshape(1, -1)\n",
    "    \n",
    "    # Jan 18th - use new loaded model and data - SCALED\n",
    "    instance = X_test_loaded_scaled.iloc[idx].values.reshape(1, -1)\n",
    "    print(f\"\\nInstance {idx + 1}:\")\n",
    "\n",
    "    \n",
    "    ########### - Do not run for experiment - ############\n",
    "    # Generate an explanation for the instance\n",
    "    #exp = explainer.explain_instance(instance, predict_fn, threshold=0.95)\n",
    "    \n",
    "    # Show the explanation in the notebook\n",
    "    #exp.show_in_notebook()\n",
    "    \n",
    "    # Generate a rescaled output to explain Anchors in actual test set values \n",
    "    #iteration_features_and_values = get_unscaled_feature_values(exp, \n",
    "    #                                                       scale_loaded, \n",
    "    #                                                       column_names)\n",
    "    #original_features_and_values\n",
    "    #display_unscaled_feature_values(iteration_features_and_values)\n",
    "    ########### - Do not run for experiment - ############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f9249-0375-46c9-842b-afe2c335123a",
   "metadata": {},
   "source": [
    "### Create an ANCHOR File Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908eed01-2b74-4f26-bbe8-39c9b56ae579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the ANCHOR results\n",
    "#anchor_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ced75-3da1-4fb6-a4ea-9dcf2786859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor_results = []\n",
    "\n",
    "# Loop through the first five instances in the test dataset\n",
    "#for idx in range(5):\n",
    "    \n",
    "    # instance = X_test_downsampled.iloc[idx].values.reshape(1, -1)\n",
    "    \n",
    "    # Jan 6th - use new model and data\n",
    "#    instance = X_test_loaded.iloc[idx].values.reshape(1, -1)\n",
    "    \n",
    "    # Generate an explanation for the instance\n",
    "#    exp = explainer.explain_instance(instance, predict_fn, threshold=0.95)\n",
    "    \n",
    "    # Extract feature importance from the explanation\n",
    "#    feature_importance = {}\n",
    "#    for condition in exp.names():\n",
    "        # Handle conditions with '='\n",
    "#        if '=' in condition:\n",
    "#            feature, value = condition.split('=')\n",
    "#            feature = feature.strip()\n",
    "#            value = float(value.strip())\n",
    "#            feature_importance[feature] = ('=', value)\n",
    "        # Handle conditions with '>' or '<'\n",
    "#        elif '>' in condition or '<' in condition:\n",
    "#            parts = re.split('([><])', condition)\n",
    "#            feature, operator, value = [part.strip() for part in parts if part.strip()]\n",
    "#            value = float(value)\n",
    "#            feature_importance[feature] = (operator, value)\n",
    "#        else:\n",
    "#            raise ValueError(f\"Unexpected format for ANCHOR explanation: {condition}\")\n",
    "\n",
    "#    anchor_results.append(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23140bb-b6cb-45a9-92c7-ed2f15233a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the results\n",
    "#df_anchor_results = pd.DataFrame(anchor_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419ce66-5465-4601-9eef-9555c4129572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrame\n",
    "#print(df_anchor_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75987bd-cb41-4576-a5d1-1b849b63978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "#df_anchor_results.to_csv('anchor_results_ANN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35439af3-1c0f-4c6a-a759-89124fd22e4d",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016d008-5f44-4e2f-8c16-34c868372354",
   "metadata": {},
   "source": [
    "# Prepare ANCHOR Values for Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9585cf24-cf79-46d4-b77d-260d8ba60253",
   "metadata": {},
   "source": [
    "## Generate Anchor Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f7c01-cb28-484e-a5ef-8b395fa925e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to limit the number of Anchor values to assess to a certain threshold\n",
    "#def limit_anchor_exps(explanation, precision_weight=0.5, coverage_weight=0.5):\n",
    "def limit_anchor_exps(condition):\n",
    "    \"\"\"\n",
    "    Calculate a score for an explanation based on its precision and coverage.\n",
    "    \n",
    "    :param explanation: An Anchor explanation object.\n",
    "    :param precision_weight: The weight given to precision in the score calculation.\n",
    "    :param coverage_weight: The weight given to coverage in the score calculation.\n",
    "    :return: A score for the explanation.\n",
    "    \"\"\"\n",
    "    #precision = explanation.precision()\n",
    "    #coverage = explanation.coverage()\n",
    "\n",
    "    # Calculate the weighted score\n",
    "    #score = (precision * precision_weight) + (coverage * coverage_weight)\n",
    "    #return score\n",
    "\n",
    "    # Example metric: favoring shorter explanations\n",
    "    return len(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58773ba7-9ff6-4231-8b30-e985f99da9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def generate_anchors_for_instances(df, num_top_exps, num_instances=2):\n",
    "    # Initialize a list to store the ANCHOR results\n",
    "    new_anchor_results = []\n",
    "    feature_instances = []\n",
    "    \n",
    "    # Loop through the first five instances in the test dataset\n",
    "    for idx in range(num_instances):\n",
    "\n",
    "        instance = df.iloc[idx]\n",
    "        feature_instances.append(instance)\n",
    "        \n",
    "        print(f'Generate Anchor exp for idx: {idx}...')\n",
    "\n",
    "        # Generate an explanation for the instance with a lower threshold\n",
    "        # The threshold and beam parameters are used reduce the generation of \n",
    "        # and Anchor set that is excessively computationally expensive\n",
    "        exp = explainer.explain_instance(instance.values.reshape(1, -1), \n",
    "                                         predict_fn, \n",
    "                                         threshold=0.99,\n",
    "                                         beam_size=1)\n",
    "\n",
    "        \n",
    "        print(f'Instance: {idx} - full exp is {exp.names()}')\n",
    "\n",
    "        # Check if an explanation was found\n",
    "        if exp is not None:\n",
    "            \n",
    "            # Score and sort the explanations, then pick the top nn\n",
    "            print(f'AAA: Number of Top Explantions {num_top_exps}')\n",
    "            sorted_conditions  = sorted(exp.names(), key=lambda x: limit_anchor_exps(x), reverse=True)[:num_top_exps]\n",
    "            print(f'Instance: {idx} - sorted exps are {sorted_conditions}')\n",
    "            \n",
    "            # Parse the conditions from the explanation and format them\n",
    "            anchor_explanation = []\n",
    "                  \n",
    "            #for condition in exp.names():\n",
    "            for condition in sorted_conditions:\n",
    "                if ' > ' in condition or ' < ' in condition:\n",
    "                    feature, relation, value = condition.split(' ')[0], condition.split(' ')[1], condition.split(' ')[2]\n",
    "                    try:\n",
    "                        anchor_explanation.append(f\"'{feature} {relation} {float(value):.2f}'\")\n",
    "                    except ValueError:\n",
    "                        anchor_explanation.append(f\"'{condition}'\")\n",
    "                else:\n",
    "                    anchor_explanation.append(f\"'{condition}'\")\n",
    "\n",
    "            # Convert the list of strings to a single string\n",
    "            anchor_explanation_str = '[' + ', '.join(anchor_explanation) + ']'\n",
    "\n",
    "            # Add the formatted explanation to the results list\n",
    "            new_anchor_results.append(anchor_explanation_str)\n",
    "        else:\n",
    "            new_anchor_results.append(\"['No explanation found']\")\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    new_df_anchor_results = pd.DataFrame(new_anchor_results, columns=['Anchor Explanation'])\n",
    "\n",
    "    # Create a DataFrame from the feature instances\n",
    "    df_feature_instances = pd.DataFrame(feature_instances)\n",
    "    \n",
    "    # Jan 6th - align index of instances to the newly created Anchor values\n",
    "    df_feature_instances = df_feature_instances.reset_index(drop=True)\n",
    "    new_df_anchor_results = new_df_anchor_results.reset_index(drop=True)\n",
    "\n",
    "        \n",
    "    return df_feature_instances, new_df_anchor_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3948cf4-ae94-4525-bde8-a92c7bcc0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_feature_instances, new_df_anchor_results = generate_anchors_for_instances(X_test_downsampled, 5)\n",
    "#results, exec_time = generate_anchors_for_instances(X_test_downsampled, 5)\n",
    "\n",
    "# Jan 6th - use new model and data\n",
    "#results, exec_time = generate_anchors_for_instances(X_test_loaded, 5)\n",
    "\n",
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "results_testdata_check, exec_time = generate_anchors_for_instances(X_test_loaded_scaled, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac31c512-9579-4066-bf50-18b90a61c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_instances, new_df_anchor_results = results_testdata_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f1944-8f9d-474f-ae0a-2694119199b6",
   "metadata": {},
   "source": [
    "## Determine Computational Efficiency Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d51de-b10b-4021-8477-f5c785549499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display time to generate DiCE explainers\n",
    "print(f\"ANCHORS Execution Time: {exec_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906265d8-c510-404e-b04b-423946e42943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrames\n",
    "#print(\"Anchor Explanations:\")\n",
    "#print(new_df_anchor_results)\n",
    "#print(\"\\nFeature Instances:\")\n",
    "#print(df_feature_instances)\n",
    "\n",
    "# Write the DataFrames to CSV files\n",
    "#new_df_anchor_results.to_csv('new_anchor_results5.csv', index=False)\n",
    "#df_feature_instances.to_csv('feature_instances5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81355e6b-81a0-47de-badb-875d6e235551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_feature_instances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da878039-4fe3-497a-8428-454144d2b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df_anchor_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae91cf1-3078-4538-a8ad-caf40f177f61",
   "metadata": {},
   "source": [
    "## Parse the Anchor Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c1d67-4887-4a47-81e2-e29b665ef984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_literal_eval(s):\n",
    "    try:\n",
    "        # Attempt to evaluate the string as a Python literal\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If there's an error, return the original string\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e171ae0-9165-4e70-97c4-e82fb266f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply safe_literal_eval to the 'Anchor Explanation' column\n",
    "#df_anchor_results_input['Anchor Explanation'] = df_anchor_results_input['Anchor Explanation'].apply(safe_literal_eval)\n",
    "\n",
    "# Step 2: Convert the 'Anchor Explanation' column from a string representation of a list back to an actual list\n",
    "new_df_anchor_results['Anchor Explanation'] = new_df_anchor_results['Anchor Explanation'].apply(ast.literal_eval)\n",
    "\n",
    "# Step 3: Determine the maximum number of conditions in the ANCHOR explanations across all instances\n",
    "max_num_conditions = max(new_df_anchor_results['Anchor Explanation'].apply(len))\n",
    "\n",
    "# Step 4: Initialize a list to store the numerical representations of the ANCHOR explanations\n",
    "numerical_explanations = []\n",
    "\n",
    "# Step 5: Loop through each ANCHOR explanation and convert it to a numerical representation\n",
    "for explanation in new_df_anchor_results['Anchor Explanation']:\n",
    "    numerical_representation = [-1] * len(df_feature_instances.columns) * max_num_conditions\n",
    "    for idx, condition in enumerate(explanation):\n",
    "        # Parse the condition to extract the feature name and value\n",
    "        feature, relation, value = condition.split(' ')[0], condition.split(' ')[1], condition.split(' ')[2]\n",
    "        \n",
    "        # Find the index of the feature in the feature dataframe\n",
    "        feature_idx = df_feature_instances.columns.get_loc(feature)\n",
    "        \n",
    "        # Store the feature index in the numerical representation\n",
    "        numerical_representation[feature_idx * max_num_conditions + idx] = float(value)\n",
    "    numerical_explanations.append(numerical_representation)\n",
    "\n",
    "# Step 6: Create a dataframe from the numerical representations\n",
    "df_anchors_numerical = pd.DataFrame(numerical_explanations)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "#print(df_anchors_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1adc6d6-3544-43eb-80e6-950d11936023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_anchor_exps(df_anchor_results_input):\n",
    "    \n",
    "    # Step 1: Convert the 'Anchor Explanation' column from a string representation of a list back to an actual list\n",
    "    df_anchor_results_input['Anchor Explanation'] = df_anchor_results_input['Anchor Explanation'].apply(ast.literal_eval)\n",
    "\n",
    "    # Step 2: Determine the maximum number of conditions in the ANCHOR explanations across all instances\n",
    "    max_num_conditions = max(df_anchor_results_input['Anchor Explanation'].apply(len))\n",
    "\n",
    "    # Step 3: Initialize a list to store the numerical representations of the ANCHOR explanations\n",
    "    numerical_explanations = []\n",
    "    \n",
    "    # Debug step\n",
    "    print(f'The value for max_num_conditions is : {max_num_conditions}')\n",
    "\n",
    "    # Step 4: Loop through each ANCHOR explanation and convert it to a numerical representation\n",
    "    for explanation in df_anchor_results_input['Anchor Explanation']:\n",
    "        \n",
    "        numerical_representation = [-1] * len(df_feature_instances.columns) * max_num_conditions\n",
    "        \n",
    "        for idx, condition in enumerate(explanation):\n",
    "            # Parse the condition to extract the feature name and value\n",
    "            feature, relation, value = condition.split(' ')[0], condition.split(' ')[1], condition.split(' ')[2]\n",
    "\n",
    "            # Find the index of the feature in the feature dataframe\n",
    "            print(f'Explanation : {explanation} for Feature {feature}')\n",
    "            feature_idx = df_feature_instances.columns.get_loc(feature)\n",
    "\n",
    "            # Print debug\n",
    "            print(f'Feature Index {feature_idx * max_num_conditions + idx}')\n",
    "            print(f'The float(value) is : {float(value)}')\n",
    "            \n",
    "            # Store the feature index in the numerical representation\n",
    "            numerical_representation[feature_idx * max_num_conditions + idx] = float(value)\n",
    "            \n",
    "        numerical_explanations.append(numerical_representation)\n",
    "\n",
    "    # Step 5: Create a dataframe from the numerical representations\n",
    "    df_anchors_numerical = pd.DataFrame(numerical_explanations)\n",
    "\n",
    "    # Display the resulting dataframe\n",
    "    #print(df_anchors_numerical)\n",
    "    \n",
    "    return df_anchors_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6ccf1-fcdb-4ab6-a9b6-8d985a56b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_anchor_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf231a-e1d5-484d-a9d1-3f4d1179a200",
   "metadata": {},
   "source": [
    "## Parse the Anchor Explanations - Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5299089-1fc3-42b3-b5fa-159bb67777ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_anchor_exps_two(df_anchor_results_input, df_feature_instances_input):\n",
    "    \n",
    "    # Step 1: Convert the 'Anchor Explanation' column from a string representation of a list back to an actual list\n",
    "    #df_anchor_results_input['Anchor Explanation'] = df_anchor_results_input['Anchor Explanation'].apply(ast.literal_eval)\n",
    "    \n",
    "    # Step 1: Convert the 'Anchor Explanation' column from a string representation of a list back to an actual list\n",
    "    # Apply safe_literal_eval to the 'Anchor Explanation' column\n",
    "    df_anchor_results_input['Anchor Explanation'] = df_anchor_results_input['Anchor Explanation'].apply(safe_literal_eval)\n",
    "\n",
    "    # Step 2: Determine the maximum number of conditions in the ANCHOR explanations across all instances\n",
    "    max_num_conditions = max(df_anchor_results_input['Anchor Explanation'].apply(len))\n",
    "\n",
    "    # Step 3: Initialize a list to store the numerical representations of the ANCHOR explanations\n",
    "    numerical_explanations = []\n",
    "    # Add another list store for 2nd values\n",
    "    numerical_explanations_2ndValue = []\n",
    "    \n",
    "    # Debug step\n",
    "    print(f'A:The value for max_num_conditions is : {max_num_conditions}')\n",
    "    \n",
    "    # create df to capture second values\n",
    "    df_secondvalues = pd.DataFrame()\n",
    "    i2ndValCnt = 1\n",
    "    \n",
    "    # Step 4: Loop through each ANCHOR explanation and convert it to a numerical representation\n",
    "    for explanation in df_anchor_results_input['Anchor Explanation']:\n",
    "        \n",
    "        print(f'B: explanation is {explanation}')\n",
    "        \n",
    "        # The anchor output dataframe needs to match the feature dataframe in length\n",
    "        # numerical_representation = [-1] * len(df_feature_instances_input.columns) * max_num_conditions\n",
    "        numerical_representation = [-1] * len(df_feature_instances_input.columns) \n",
    "        numerical_representation_2ndNum = [-1] * len(df_feature_instances_input.columns) \n",
    "        \n",
    "        print(f'C:numerical_representation is : {numerical_representation}')\n",
    "        print(f'D:numerical_representation lenght is : {len(numerical_representation)}')\n",
    "        \n",
    "        print(f'E:numerical_representation_2ndNum is : {numerical_representation_2ndNum}')\n",
    "        print(f'F:numerical_representation_2ndNum lenght is : {len(numerical_representation_2ndNum)}')\n",
    "        \n",
    "        for idx, condition in enumerate(explanation):\n",
    "            \n",
    "            print(f\"1:Condition to parse: {condition}\")\n",
    "            \n",
    "            # Regular expression to capture feature name between two numeric values\n",
    "            match_between = re.search(r'(-?\\d+\\.\\d+|-?\\d+)\\s*<\\s*([A-Za-z_.]+)\\s*<=\\s*(-?\\d+\\.\\d+|-?\\d+)', condition)\n",
    "\n",
    "            # Regular expression to capture feature name followed by a numeric value\n",
    "            #match_after = re.search(r'([A-Za-z_.]+)\\s*[<>=]+\\s*(-?\\d+\\.\\d+|-?\\d+)', condition)\n",
    "            \n",
    "            # Try to match the pattern: feature <= number\n",
    "            #match_after = re.search(r'([A-Za-z_.]+)\\s*<=?\\s*(-?\\d+\\.\\d+|-?\\d+)', condition)\n",
    "            match_after = re.search(r'([A-Za-z0-9_.]+)\\s*([<>=]+)\\s*(-?\\d+.\\d+|-?\\d+)', condition)\n",
    "\n",
    "            feature, value, value2 = None, None, None\n",
    "\n",
    "            if match_between:\n",
    "                print('1a:Match Between')\n",
    "                value = float(match_between.group(1))\n",
    "                feature = match_between.group(2)\n",
    "                value2 = float(match_between.group(3))\n",
    "            elif match_after:\n",
    "                print('1b:Match After')\n",
    "                feature = match_after.group(1)\n",
    "                value = float(match_after.group(3))\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Parse the condition to extract the feature name and value\n",
    "            #feature_old, relation, value_old = condition.split(' ')[0], condition.split(' ')[1], condition.split(' ')[2]\n",
    "\n",
    "            # Capture segments with numbers, possibly preceded by non-numeric characters\n",
    "            #segments = re.findall(r'([A-Za-z_.]*-?\\d+\\.\\d+|[A-Za-z_.]*-?\\d+)', condition\n",
    "            #segments = re.findall(r'([A-Za-z_.]*-?\\d+\\.\\d+|[A-Za-z_.]*-?\\d+)', condition)\n",
    "            #segments = re.findall(r'(-?\\d+\\.\\d+|-?\\d+|[A-Za-z_.]+)', condition)\n",
    "            #segments = re.findall(r'(-?\\d+\\.\\d+|-?\\d+|\\D+)', condition)\n",
    "            \n",
    "                                  \n",
    "            # Extract the feature name and the first numerical value from the segments\n",
    "            #if segments:\n",
    "            #    print('Segments found\\n')\n",
    "            #    feature, value, value2 = None, None, None\n",
    "                #found_first_value = False                  \n",
    "            #    for segment in segments:\n",
    "                    # If a segment starts with non-numeric characters, it's likely the feature name\n",
    "                    # Check if the segment is a numeric value\n",
    "            #        if re.match(r'-?\\d+\\.\\d+|-?\\d+', segment):\n",
    "            #            if value is None:\n",
    "            #                value = float(segment)\n",
    "            #        elif value2 is None:\n",
    "            #            value2 = float(segment)\n",
    "            #    else:\n",
    "                    # Remove any non-alphanumeric characters from feature name candidates\n",
    "            #        possible_feature = re.sub(r'[^A-Za-z0-9_.]', '', segment)\n",
    "            #        if possible_feature and possible_feature in df_feature_instances.columns:\n",
    "            #            feature = possible_feature\n",
    "\n",
    "            #values = re.findall(r'-?\\d+\\.\\d+|-?\\d+', condition)\n",
    "            #values = re.findall(r'(?<=^|[\\s<>=])-?\\d+\\.\\d+|-?\\d+', condition)\n",
    "            \n",
    "            # Check if there is more than one numerical value and display them\n",
    "            if value2 != None:\n",
    "                print(f\"2:Multiple numerical values found in condition '{condition}':\")\n",
    "                print(f\"2:Value2 is : {value2}\")\n",
    "                    \n",
    "            # Chack feature extraction from Condition\n",
    "            #feature3 = re.sub(r'[\\d<>=. -]', '', condition).strip()\n",
    "            print(f\"3:Extracted feature value - feature -: {feature}\")\n",
    "            \n",
    "            print(f'4:The extracted value is : {value}')\n",
    "            \n",
    "            # Correct for zero errors with value\n",
    "            if not value:\n",
    "                print('4a:Trying to correct - value..')\n",
    "                value = 0.0\n",
    "\n",
    "            \n",
    "            # Handle unexpected formats or values\n",
    "            #if not value or not feature or feature not in df_feature_instances_input.columns:\n",
    "            if not feature or feature not in df_feature_instances_input.columns:    \n",
    "                if not value:\n",
    "                    print('NOT Value')\n",
    "                if not feature:\n",
    "                    print('NOT Feature')\n",
    "                if feature not in df_feature_instances_input.columns:\n",
    "                    print('Feature name not found in column list')\n",
    "                print(f\"4b:Unexpected format for condition: {condition}\")\n",
    "                continue\n",
    "\n",
    "            # Assign the first numerical value to the feature\n",
    "            #value = float(values[0])\n",
    "            \n",
    "            # Find the index of the feature in the feature dataframe\n",
    "            print(f'5:Explanation : {explanation} for Feature {feature}')\n",
    "            feature_idx = df_feature_instances_input.columns.get_loc(feature)\n",
    "\n",
    "            # Print debug\n",
    "            print(f'6:Feature Index - feature_idx * max_num_conditions + idx : {feature_idx * max_num_conditions + idx}')\n",
    "            print(f'6:Feature Index - feature_idx + idx : {feature_idx + idx}')\n",
    "            print(f'6:The float(value) is : {float(value)}')\n",
    "            \n",
    "            print(f'6b:[feature_idx] is : {feature_idx}')\n",
    "            print(f'6b:[idx] is : {idx}')\n",
    "            \n",
    "            print(f'7a:[feature_idx * max_num_conditions + idx] is : {feature_idx * max_num_conditions + idx}')\n",
    "            print(f'7b:[feature_idx + idx] is : {feature_idx + idx}')\n",
    "            print(f'7c:[feature_idx] is : {feature_idx}')\n",
    "            \n",
    "            # Store the feature index in the numerical representation list\n",
    "            #numerical_representation[feature_idx * max_num_conditions + idx] = float(value)\n",
    "            #numerical_representation[feature_idx + idx] = float(value)\n",
    "            numerical_representation[feature_idx] = float(value)\n",
    "                                  \n",
    "            # Additional processing can be done with value2 if needed\n",
    "            if value2 != None:\n",
    "                print('8: Second value stuff happening...1')\n",
    "                # Add second value to secondary list\n",
    "                numerical_representation_2ndNum[feature_idx] = float(value2)\n",
    "                # Create a new column in which to eventually store the second value\n",
    "                df_secondvalues[feature +'_Anchor' + str(i2ndValCnt)] = -1\n",
    "                i2ndValCnt +=1\n",
    "            else:\n",
    "                # If no second value then pad out with a 'dummy'\n",
    "                numerical_representation_2ndNum[feature_idx] = -1         \n",
    "            \n",
    "        numerical_explanations.append(numerical_representation)\n",
    "        # A list generated for 2nd values in Anchor conditions\n",
    "        numerical_explanations_2ndValue.append(numerical_representation_2ndNum)\n",
    "\n",
    "    # Step 5: Create a dataframe from the numerical representations\n",
    "    df_anchors_numerical = pd.DataFrame(numerical_explanations)\n",
    "    # Account for second value in expression\n",
    "    df_anchors_numerical_2ndValue = pd.DataFrame(numerical_explanations_2ndValue)\n",
    "    \n",
    "\n",
    "    # Display the resulting 2nd value dataframe\n",
    "    print('\\ndf_anchors_numerical_2ndValue...before removing...')\n",
    "    print(df_anchors_numerical_2ndValue)\n",
    "    \n",
    "    # Remove 2nd value columns where all values are the same\n",
    "    #columns_to_drop = [col for col in df_anchors_numerical_2ndValue.columns if df_anchors_numerical_2ndValue[col].nunique() == 1]\n",
    "    #df_anchors_numerical_2ndValue.drop(columns=columns_to_drop, inplace=True)\n",
    "    df_anchors_numerical_2ndValue = df_anchors_numerical_2ndValue.loc[:, df_anchors_numerical_2ndValue.nunique() != 1]\n",
    "    \n",
    "    # Display the resulting 2nd value dataframe\n",
    "    print('\\ndf_anchors_numerical_2ndValue...after removing...')\n",
    "    print(df_anchors_numerical_2ndValue)    \n",
    "    \n",
    "    \n",
    "    # Copy the results of the second values into the dataframe labelled with new columns\n",
    "    # df_secondvalues.iloc[:,:] = df_anchors_numerical_2ndValue.values\n",
    "    \n",
    "    # Add code here to append the second values df to both the anchors and features dataframe outputs\n",
    "    print(f'\\nshape of df_secondvalues: {df_secondvalues.shape}')\n",
    "    #df_secondvalues.shape\n",
    "    # ....\n",
    "    # ....\n",
    "    \n",
    "    \n",
    "    # Display the dataframe with 2nd value columns\n",
    "    print('\\ndf_secondvalues...')\n",
    "    print(df_secondvalues)\n",
    "    \n",
    "    return df_feature_instances_input, df_anchors_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b3603-1b51-44fb-bec2-9649f2b2c8a2",
   "metadata": {},
   "source": [
    "## Display ANCHORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26960e2-6520-4a42-8e8d-6924aa02182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of each dataset to understand their structure\n",
    "instance_features_head = df_feature_instances.head()\n",
    "anchor_explanations_head = new_df_anchor_results.head()\n",
    "anchor_explanations_numerical = df_anchors_numerical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4656447d-1e45-4aee-923e-fea4e1cf58d1",
   "metadata": {},
   "source": [
    "## Generate Outfile for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f2617-a30a-4afd-adf7-cc50712be650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#anchors_num_explainers_filepath = \"anchor_numerical_explainers.csv\"\n",
    "#anchor_explanations_numerical.to_csv(anchors_num_explainers_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7328d49-c5d1-4a54-b589-39af23ef753d",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8089e3-25a2-4d31-a561-54b320557052",
   "metadata": {},
   "source": [
    "# XAI Experiments - Metrics Capture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbfd08-0e71-4e03-9d22-6ca1f60c36c1",
   "metadata": {},
   "source": [
    "## Suppress Warnings to clean up output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779129cb-fe2b-48b2-9f4b-d502bd8c2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06edb9a-6c43-4b40-a58c-55da8fb7109f",
   "metadata": {},
   "source": [
    "## Break out Model Test Data into a list of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc5411-844e-4633-b541-4505316305bf",
   "metadata": {},
   "source": [
    "### Create Test Data for Experiment Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524d948-928b-400e-a839-4e450e1af6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_feature_names = [col for col in df_downsampled_loaded.columns if col != 'Fraud']\n",
    "\n",
    "# Ensure X_test_loaded has the correct column names (if necessary)\n",
    "#X_test_loaded.columns = original_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd2cab-0181-46c6-a7cb-1649033b43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "# Convert NumPy array to DataFrame\n",
    "X_test_loaded_scaled = pd.DataFrame(X_test_loaded_scaled, columns=original_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a162404-158f-40a1-a3ef-0ddb54ecc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X_test_loaded and y_test into a single DataFrame\n",
    "#df_TestData = pd.concat([X_test_loaded, y_test_loaded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1ae77-f298-4834-9104-fb1403374326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "# Combine X_test_loaded and y_test into a single DataFrame\n",
    "df_TestData = pd.concat([X_test_loaded_scaled, y_test_loaded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18990dd-42e7-42b5-8e1c-a7cc455183fa",
   "metadata": {},
   "source": [
    "### Split the DataFrame into 20 consecutive smaller DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2c609-8229-4057-a0c9-7e110646843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into 20 consecutive smaller DataFrames\n",
    "split_size, list_df = split_TestData_into_nn_Blocks(df_TestData, num_splits = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8184b5ca-e8c8-4f1a-8452-218cf3d8458e",
   "metadata": {},
   "source": [
    "### Check Label Count for Stability Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af9224-634b-4402-bca8-c483dbb24f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrence of each unique value in the 'Fraud' column\n",
    "fraud_counts = df_TestData['Fraud'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Breakdown of 'Fraud' and non-Fraud label records in df_TestData:\")\n",
    "print(fraud_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe0f01a-d8cf-4cc4-a158-8d0f2e234351",
   "metadata": {},
   "source": [
    "### Add a routine to check output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc91e06-63ae-412a-a2c3-b6d0c789d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display starting points in the first nn sub dataframes\n",
    "startBlockDisplay(df_TestData, split_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e8f6db-a6ae-4c0b-bc9f-6d9ef74568c7",
   "metadata": {},
   "source": [
    "## Confirm Starting Point in External ANCHORS XAI XL File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346f0bf-0d71-4a47-9930-f78494b34134",
   "metadata": {},
   "source": [
    "The code below acts so that for each dataframe in the list just created the following actions are carried out;\n",
    "\n",
    "Check if an XAI results XL spreadsheet called 'ANCHOR_XAI_Metrics_Experiments.xls' exists;\n",
    "\n",
    "If not create an empty XL spreadsheet with the name 'ANCHOR_XAI_Metrics_Experiments.xls', and then define a variable called âSampleâ with an integer value of 1 and print the value of 'Sample' to output.\n",
    "\n",
    "If and XL spreadsheet called 'ANCHOR_XAI_Metrics_Experiments.xls' does exist, then read the entries in the spreadsheet in the first column named âSample Numberâ and create a variable in this Python program named âSampleâ that is one integer value higher than the highest integer number column named âSample Numberâ in the XL, and print this value of 'Sample' to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb5690-396c-4afc-8d5a-9b23f273b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential number as an identifier for each DataFrame\n",
    "list_df = {f'df_{i + 1}': list_df[i] for i in range(len(list_df))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653288a-3868-419c-a0e5-ac6ba3914ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the ANCHOR XAI metrics results spreadsheet\n",
    "ANCHOR_xai_file_path = 'ANCHOR_XAI_Metrics_Experiments.xlsx'  # Stored locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5cb374-7e77-49b6-ac0a-e72fafe384ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Function to update or create the spreadsheet and determine the 'Sample' number\n",
    "# Process each dataframe in 'list_df'\n",
    "sample = return_next_sample_number_to_process(list_df, ANCHOR_xai_file_path, \"ANCHOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1351ec4-6420-41be-bc9a-bbe554fdf773",
   "metadata": {},
   "source": [
    "## Select Next Dataframe to Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6338def4-4a6d-43ad-b4db-430dfac204c6",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "\n",
    "\t\n",
    "Extend the Python code so that the code reads in the dataframe from 'list df' that corresponds to the integer value in the \n",
    "variable named âSampleâ. \n",
    "\n",
    "Assign this dataframe the name 'df_Selected_from_List'.\n",
    "\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2191ad-fab5-49d8-a1b7-483de071ded8",
   "metadata": {},
   "source": [
    "### Initialize Dataframe to Capture Re-start Point as None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7003e-560f-4327-b597-16d8fa497976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize df_Selected_from_List as None\n",
    "df_Selected_from_List = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b362c7-d214-4974-83e6-1604c0e505c9",
   "metadata": {},
   "source": [
    "### Extract test data block to restart XAI metrics process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f305ad-68cc-4d29-8d58-f7269da5ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Selected_from_List, key = select_restart_testdata_block(df_Selected_from_List, \n",
    "                                                           list_df, \n",
    "                                                           ANCHOR_xai_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d2b62-48a3-4c94-9c37-6d3220d45d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no DataFrame is selected (e.g., if 'Sample' exceeds the number of DataFrames in list_df)\n",
    "if 'df_Selected_from_List' not in locals():\n",
    "    print(\"No DataFrame selected. The 'Sample' number may exceed the number of DataFrames in list_df.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43df2c1-f877-4f68-ba62-ff5273add48e",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f61762-1dda-486a-9fc5-02efa921aa63",
   "metadata": {},
   "source": [
    "## Generate XAI Metrics from Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575583cc-5b01-4d96-9741-b46e9d783497",
   "metadata": {},
   "source": [
    "### Generate the ANCHOR Values for the Test Data Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5aa4c-392a-4b3b-8bdf-3d979e9bc3eb",
   "metadata": {},
   "source": [
    "#### Pre-Check Values for Data Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6461d79-b57d-450e-94db-32189cd0caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Selected_from_List.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aaf165-fcb1-4f2d-bfd2-3ff0d1bdd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_Selected_from_List.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecddff26-6b82-4cb4-a9df-20f69a3636a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Selected_from_List.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c2b0f-b537-496c-ade6-3c756e996a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_Selected_from_List.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e009c9-6632-4113-a6cf-fb111e7a5a4f",
   "metadata": {},
   "source": [
    "#### Get Label Values for Stability Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48fa02-74d4-4e46-a5ef-aedff1aa1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_block_labels_df = df_Selected_from_List['Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9929944-4bd1-45e6-8a5d-b32c8858fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_block_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49a855-5f7e-404e-b1a4-73a9882024b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_block_labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ab65b-4d0b-492c-af94-40745c9735a3",
   "metadata": {},
   "source": [
    "#### Pre-Process Values for Data Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aaf13c-1d9a-48c0-962b-c3389b034082",
   "metadata": {},
   "source": [
    "Extract the label values from the data block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad0140-860a-4d6d-a550-4022cdada039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Selected_from_List = df_Selected_from_List.drop('Fraud', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7372ed-d40d-42a4-91fd-13a757239afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_Selected_from_List.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3080e1-7491-4666-8171-781e0f8aa8df",
   "metadata": {},
   "source": [
    "Set limit value (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc0d36-dcbe-4a9f-9eec-e9a6f08d795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set limit - '64' will process the entire data block\n",
    "limit_data_block_rows = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645dc31-b9b2-4d8f-96c4-39a6c7bdbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set limit on number of Anchor explanations to process\n",
    "exp_limit = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ecaf8-9d2f-41ce-8395-021abc959595",
   "metadata": {},
   "source": [
    "#### Get Anchor Values for Data Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35657476-f385-44b7-a2d3-79bad9a4686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ANCHOR, exec_time_ANCHOR = generate_anchors_for_instances(df_Selected_from_List, \n",
    "                                                                  exp_limit, \n",
    "                                                                  limit_data_block_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f305b-24a7-4686-b4c3-14cef11194e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_anchor_instances, df_anchor_results_block = results_ANCHOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c485f2f-fbe6-49e3-8f08-d1b44c353865",
   "metadata": {},
   "source": [
    "### Parse the ANCHOR Values for the Test Data Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba73b2-5183-4387-a2ac-7bc277d3cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_anchor_instances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d9eb9-3ed1-43ea-9969-d60d4d60ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anchor_results_block.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10735d7-39b4-4b21-972f-46c5ed1be329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_anchor_instances, df_anchors_numerical_parsed = parse_anchor_exps_two(df_anchor_results_block,\n",
    "                                                    #df_feature_instances)\n",
    "                                                    df_feature_anchor_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f70ec-8dd9-41a0-8293-2db2d5fcfc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anchors_numerical_parsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38441c32-0017-461c-8ee8-b0f6bc9fcf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_anchor_instances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd6cd0-996c-415a-ba95-33f14c52aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the feature file to csv\n",
    "df_feature_anchor_instances.to_csv('df_feature_anchor_instances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee1601-3a66-470a-b279-cd8f2bbbe649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the xai generated files to csv\n",
    "df_anchors_numerical_parsed.to_csv('df_anchors_numerical_parsed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540fe51-66df-4f98-96bb-2980a5c7a02a",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb1b263-2be1-4f49-b121-7460a580745a",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0114b8-2ed0-431f-8a83-05f05a2003c4",
   "metadata": {},
   "source": [
    "### Generate Identity Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a45e4-d979-4ea3-8076-cc6c7917f0fb",
   "metadata": {},
   "source": [
    "#### Run a Basic Test First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50bd85-3080-46c4-9846-c3e36c468213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two random instances from the ANCHOR dataframe\n",
    "df_xai_numerical = df_anchors_numerical_parsed\n",
    "\n",
    "random_indices = np.random.choice(df_xai_numerical.index, size=2, replace=False)\n",
    "instance_1 = df_xai_numerical.iloc[random_indices[0]]\n",
    "instance_2 = df_xai_numerical.iloc[random_indices[1]]\n",
    "\n",
    "# Compute the Euclidean distance between the selected instances - uses custom project function\n",
    "distance = get_euclidean_distance(instance_1, instance_2)\n",
    "print(f\"Euclidean distance between instance {random_indices[0]} and instance {random_indices[1]}: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59711fa-7b36-4c00-98a8-d7aff50469ef",
   "metadata": {},
   "source": [
    "#### Retrieve Identity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f8eb2-a4b6-4c9c-9ef9-5abbd0681884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anchors_numerical_parsed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c037a-30ed-4008-93df-03b8a74409b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_anchors_numerical_parsed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7bbae-661a-4e26-8f15-0ab6e1a131c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_anchor_instances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8128d-7e62-4b8a-bd56-6633e13162ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_feature_anchor_instances.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f36167-26e5-49ab-a94f-1713df0163c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "ANCHOR_Identity_Metric = get_identity_metric(df_feature_anchor_instances, \n",
    "                                             df_anchors_numerical_parsed, \n",
    "                                             \"ANCHOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1ad83-911c-4af0-b296-4dd1f27ed63c",
   "metadata": {},
   "source": [
    "#### Display Identity Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d4d91-7a13-4637-876c-95a0fb3e390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Identity_Number = \"{:.2f}%\".format(ANCHOR_Identity_Metric)\n",
    "display_text(\"ANCHOR Identity Metric Score: \" + ANCHOR_Identity_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22956d72-c9c3-45ff-85ed-11dedabf3c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in XAI Metric for Identity\n",
    "XAI_Ident_Metric_1 = ANCHOR_Identity_Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937dd6e-9301-4efc-ad80-bf2b88c25e66",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f32ef-ea85-4594-9f7b-3dcd0baa7615",
   "metadata": {},
   "source": [
    "### Generate Stability Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af639c6d-16ad-4550-adc2-0bb5297ccad3",
   "metadata": {},
   "source": [
    "#### Pre-Processing of Stability Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15de4e-b6fa-4b45-b6d3-a65679d8b4ee",
   "metadata": {},
   "source": [
    "Check Test Set Labels are correctly indexed for Stability Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9630f3-1134-4965-b088-d3a1af8b404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_anchor_instances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d487ff8-85e6-46cd-a683-fc52de1ffa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anchors_numerical_parsed.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc15a79-72e5-4209-956b-d1e4ff22f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c77c0-98e8-4418-ad8d-b237d40d0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_block_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a40a20-ceef-4c3e-97b7-473006fb82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y_test_block_labels_df')\n",
    "print(y_test_block_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b341cbc-ceb2-41f7-9308-3c8e14af577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the label value input to match earlier adjustments in DiCE value creations\n",
    "if limit_data_block_rows > 0:\n",
    "    y_test_block_labels_df = y_test_block_labels_df.iloc[:limit_data_block_rows]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900cf1d-b374-4e94-9148-8b646a0ce231",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_block_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b6cff0-02f1-4bf0-89aa-508a669abd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the column name 'Fraud'\n",
    "y_test_block_labels_df.columns = ['Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc8469-e324-48d5-8455-1fb08096add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_block_labels_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1cfe4-df18-476a-b481-69d0a93004d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_block_labels_df = y_test_block_labels_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f812d6-1c99-411d-807b-ca298f822cdf",
   "metadata": {},
   "source": [
    "Determine which label is most common in current data block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea844f-d6d7-49bc-a395-93fd13c3bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LabelCount = pd.DataFrame(y_test_block_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e861d-73af-4394-9f1f-e3e90ba654ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrence of each unique value in the 'Fraud' column\n",
    "fraud_counts_label = df_LabelCount['Fraud'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Breakdown of 'Fraud' and non-Fraud label records in df_TestData:\")\n",
    "print(fraud_counts_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1cd63-7994-49b5-8a99-d7b597ff5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the label with the most entries\n",
    "largest_label = fraud_counts_label.idxmax()\n",
    "\n",
    "# Assigning it to largest_label_count\n",
    "#largest_label_count = label_counts[largest_label]\n",
    "largest_label_count = fraud_counts_label[largest_label]\n",
    "\n",
    "print(\"Label with most entries:\", largest_label)\n",
    "print(\"Count of this label:\", largest_label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe2db4-68fa-4a50-9d3e-81f5ec72823a",
   "metadata": {},
   "source": [
    "#### Retrieve Stability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf065d-804c-4454-80c8-334049849909",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Stability_Metric = get_stability_metric_y(df_anchors_numerical_parsed, \n",
    "                                                 y_test_block_labels_df,\n",
    "                                                 largest_label, \n",
    "                                                 'ANCHOR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1060608-b794-4073-9751-df1b3294c3f9",
   "metadata": {},
   "source": [
    "#### Display Stability Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc4603b-b0d2-43e9-933e-37cce324705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Stability_Number = \"{:.2f}%\".format(ANCHOR_Stability_Metric)\n",
    "display_text(\"ANCHOR Stability Metric Score: \" + ANCHOR_Stability_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5d620-febe-4aae-b1e6-1ccd403cf3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in XAI Metric for Stability\n",
    "XAI_Stability_Metric_2 = ANCHOR_Stability_Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820448e-54dd-4818-8bcc-91b047b7ac5e",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74a317f-c4af-4639-80f2-38fad643badc",
   "metadata": {},
   "source": [
    "### Generate Seperability Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40a94b-a2a5-4666-9344-2a4031c17ecf",
   "metadata": {},
   "source": [
    "#### Retrieve Seperability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245d122-3075-4b17-93be-ecd1c672d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_feature_anchor_instances.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a04078-3546-4468-a601-5843a741a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_anchors_numerical_parsed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f819e-4d24-489d-b4b5-7106e19e7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_feature_anchor_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87b211-76db-4341-bf71-0046b39630ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_anchors_numerical_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4620a-9000-426f-b04d-091884c57e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Seperability_Metric = get_seperability_metric(df_feature_anchor_instances, \n",
    "                                                     df_anchors_numerical_parsed, \n",
    "                                                     \"ANCHOR\",\n",
    "                                                     0.9687, # threshold  #0.51  0.80  0.99\n",
    "                                                     0.05) # tolerance) #0.35  0.01  0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344e2cc-ca46-450c-9b4d-d4fb7e4d513f",
   "metadata": {},
   "source": [
    "#### Display Seperability Score MetricÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1886d-bea5-476f-923c-09015f49c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Seperability_Number = \"{:.2f}%\".format(ANCHOR_Seperability_Metric)\n",
    "display_text(\"ANCHOR Seperability Metric Score: \" + ANCHOR_Seperability_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ba1e4-624b-409d-8907-7f9a621287c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in XAI Metric for Seperability\n",
    "XAI_Seperability_Metric_3 = ANCHOR_Seperability_Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8e76a-2713-4432-9e6d-9b2483ea5d85",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d8eda-b31c-4bc6-84de-47575bd12f4c",
   "metadata": {},
   "source": [
    "### Generate Similarity Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f19e03-ba10-4d07-b8da-a6acb5076310",
   "metadata": {},
   "source": [
    "#### Retrieve Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81f748-4172-41cd-abff-3ec254fd4fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_feature_anchor_instances.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5ff72-d71a-4ac9-b7f8-95fc705713ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_anchors_numerical_parsed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988de3c-ff88-4948-aca1-4d717ab9a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Similarity_Metric = get_similarity_metric(df_feature_anchor_instances, \n",
    "                                                 df_anchors_numerical_parsed, \n",
    "                                                 \"ANCHOR\", \n",
    "                                                 use_dbscan=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c0bd9-04c1-4d33-9ba7-e052bfbf4f25",
   "metadata": {},
   "source": [
    "#### Display Similarity Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0d8ab-a4e2-4bee-9e60-75800053338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_Similarity_Number = \"{:6.2f}\".format(ANCHOR_Similarity_Metric)\n",
    "display_text(\"ANCHOR Similarity Metric Value: \" + ANCHOR_Similarity_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d7e3a-5e09-4ca7-a06c-4374e4f060c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in XAI Metric for Similarity\n",
    "XAI_Similarity_Metric_4 = ANCHOR_Similarity_Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69674cf-e8ca-4166-abbf-558e9d59804c",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52602c42-bd19-442e-b371-74f677c552e1",
   "metadata": {},
   "source": [
    "### Display Final Set of Metrics (this run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11590dd6-44ae-4e59-bc2c-23d8aa62542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(f\"XAI Ident Metric 1: {XAI_Ident_Metric_1}\")\n",
    "print(f\"XAI Stability Metric 2: {XAI_Stability_Metric_2}\")\n",
    "print(f\"XAI Seperability Metric 1: {XAI_Seperability_Metric_3}\")\n",
    "print(f\"XAI Similarity Metric 1: {XAI_Similarity_Metric_4}\")\n",
    "print(f\"XAI Time Metric 5: {exec_time_ANCHOR} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff39cc-b050-48f8-94a6-00a0912e2095",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151b620-c616-4ffb-8c76-6b99a9f11609",
   "metadata": {},
   "source": [
    "## Write Out Metrics to XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93144f1-2731-4dc1-9cd7-19b233444ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_xai_Metrics_to_XL(ANCHOR_xai_file_path, \n",
    "                        sample, \n",
    "                        ANCHOR_Identity_Metric, \n",
    "                        ANCHOR_Stability_Metric, \n",
    "                        ANCHOR_Seperability_Metric, \n",
    "                        ANCHOR_Similarity_Metric, \n",
    "                        exec_time_ANCHOR, \n",
    "                        df_Selected_from_List,\n",
    "                        \"ANCHOR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e27ea0-eeec-44c9-8d65-2de1617047e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
