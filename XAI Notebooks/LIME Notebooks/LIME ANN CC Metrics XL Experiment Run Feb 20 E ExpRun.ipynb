{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6a486b-7a72-4691-8f57-260d79c5b234",
   "metadata": {},
   "source": [
    "# Disseration Experiment 4k\n",
    "# Generate SHAP XAI Output (Credit Card Fraud) - Feb 18¶\n",
    "Ciaran Finnegan February 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55bfa4-8cba-4a30-b0d9-8bd96067b62c",
   "metadata": {},
   "source": [
    "# Import Libraries + Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22db83-0610-4b09-ba71-4a100adbe52d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884cf21-2d0d-457b-b738-8a9280059051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import Display libraries\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "import raiutils\n",
    "from raiutils.exceptions import UserConfigValidationException\n",
    "\n",
    "\n",
    "# Import necessary libraries for ANN model building\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Import necessary libraries for LIME calculations\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Libraries required for metrics calculations\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Classifier training (not used for explainability)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Libraries used in Experiment Creation of XL Output Metrics\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dd957-f6e9-426e-bf48-eeed4da35013",
   "metadata": {},
   "source": [
    "## Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0a5fb-5c62-4b71-87a4-e357763e7596",
   "metadata": {},
   "source": [
    "Dataset Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee182d-e98c-4502-8e8c-887566086684",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./DS_Visualisation_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c876e3-7193-45ea-b1b8-7266f0282857",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d642c-b199-4770-ba5f-c6ab65b4d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./XAI_Metrics_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13ae7a-5810-4fc5-a306-1316afe7e36c",
   "metadata": {},
   "source": [
    "Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c8a1b-4941-4b6d-bb45-d1f703adfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./DS_Model_Build_Evaluation_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503e5d0-7f4e-443e-9c77-99cf14c8df4c",
   "metadata": {},
   "source": [
    "Track Experiment Result Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e12fd-a422-498f-94e3-3a921a16d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./XAI_Experiment_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e8334-6afb-43d7-9f06-ef6eb7197751",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d4c869-7f34-44bf-b7c6-02f55742df14",
   "metadata": {},
   "source": [
    "A Neural Network Model has been created in another Kubeflow Notebook and is being used in all the XAI experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682526b-ab65-4d9b-a9e9-5268d57ac331",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('ccfraud_model')  # If saved as SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5c46b-09b3-4794-a6a3-9ad43aef4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_loaded, y_test_loaded, X_train_loaded, y_train_loaded, df_downsampled_loaded, dfCatCols = load_CC_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c2706-4fc6-489d-96b4-2f06f6fe5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_loaded.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a12c1-cbe3-4839-b8f0-8b9e823684b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_loaded.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177fa0b-2a10-4d58-9208-cb32dc5d8a67",
   "metadata": {},
   "source": [
    "## Re-Display Model Peformance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d8bbd3-d74b-45a7-a62b-be77e2d33c39",
   "metadata": {},
   "source": [
    "For illustration, the evualtion metrics of the NN model will be repeated here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0388a-335c-4476-92d2-36c46190f503",
   "metadata": {},
   "source": [
    "### Re-Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3289e8-6a06-4ba7-8d23-29a103681c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_loaded_scaled, X_test_loaded_scaled, scale_loaded = scale_the_features(X_train_loaded, \n",
    "                                                                                X_test_loaded, \n",
    "                                                                                df_downsampled_loaded, \n",
    "                                                                                'Fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426c206-217c-4afa-9e85-8ae54c0358af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set option to display all columns (you can adjust the number as needed)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b3384-cd46-4a22-8556-78c742d1c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_loaded_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0e53d-ab3b-4516-9965-dc0b5838ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_loaded_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed92ed-e146-42cf-ad22-03169d8f40b4",
   "metadata": {},
   "source": [
    "### Re-evaluate loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf97f4d-feca-4bfc-97a7-e954b25d3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_loaded = display_model_metrics_tabular(loaded_model, X_test_loaded_scaled, y_test_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944ea81-e5a7-4730-be7d-7e105303c03c",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bbe94e-6beb-4fe6-918b-fa2c22154a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(y_test_loaded, y_pred_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119542a4-b5a9-44d8-8f61-296092e9ef6b",
   "metadata": {},
   "source": [
    "# Generate LIME Values (Examples Instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1206d29-8555-46e0-8a71-2673e90e8d36",
   "metadata": {},
   "source": [
    "Set Up Column Names for Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b236ae-20b9-41b5-bc0c-00c6a189500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'X_test' is a numpy array and you have a list of the original column names\n",
    "# original_feature_names = [col for col in df_downsampled_loaded.columns if col != 'Fraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c68a0-a8ea-417d-9e29-044f02871108",
   "metadata": {},
   "source": [
    "## Select Value + Prepare Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c60e6-f70c-4ec0-86ea-617b27f78e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries - required here to ensure'lime_tabular' routines are available\n",
    "from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e089a-4bcb-409a-a366-27eb608aa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jan 6th - use new loaded model\n",
    "column_names = df_downsampled_loaded.drop('Fraud', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700737d-c454-459e-ba1a-3c9857bc74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy array to DataFrame\n",
    "X_test_loaded_scaled = pd.DataFrame(X_test_loaded_scaled, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1328a3b9-333c-4111-b478-08b7c2e34259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jan 18th - use new loaded mode, and SCALED data\n",
    "random_observation = X_test_loaded_scaled.sample(1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d7fc4-2e18-4c59-a84e-7ec3e695abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = random_observation.values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fa0b9-cbe9-4c39-967a-8bfa18ac6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function to output probabilities for both classes\n",
    "def predict_proba_wrapper(data):\n",
    "    \n",
    "    # Get the probability of the positive class\n",
    "    prob_pos = loaded_model.predict(data)\n",
    "    \n",
    "    # Get the probability of the negative class\n",
    "    prob_neg = 1 - prob_pos\n",
    "    \n",
    "    # Combine and return the probabilities\n",
    "    return np.hstack([prob_neg, prob_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd33bb4-c019-41f3-a845-8026b11b96b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy array to DataFrame\n",
    "X_train_loaded_scaled = pd.DataFrame(X_train_loaded_scaled, columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908861ac-42ea-47d9-a9c8-a4db61b1a25d",
   "metadata": {},
   "source": [
    "## Single Instance LIME Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b6e25-2f41-49dc-bae4-baade4548744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "explainer = lime_tabular.LimeTabularExplainer(X_train_loaded_scaled.values, \n",
    "                                              mode='classification',\n",
    "                                              training_labels=y_train_loaded,\n",
    "                                              class_names=['Non Fraud', 'Fraud'], \n",
    "                                              feature_names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43bd29-dfa1-480b-abdb-9562832f69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Explaining the prediction using the LIME explainer\n",
    "# Using our wrapper function for predictions\n",
    "explanation = explainer.explain_instance(instance[0], predict_proba_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd52bf-fd09-43ee-8673-caeda5cbcb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction for the instance\n",
    "prediction_probs1 = predict_proba_wrapper(instance[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4908ac-65d4-4087-8e26-db5a67fdd33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Predicted Probability (Fraud): {prediction_probs1[0][1]:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a9b34-c372-4cbd-95d1-fd31b3cf0bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the LIME explanations for the instance\n",
    "# 'show_in_notebook' displays the explanation in a Jupyter notebook format\n",
    "explanation.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d2592-fe45-4159-9327-f7f623e9232e",
   "metadata": {},
   "source": [
    "## Build LIME Explainers for XAI Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c5a90-cfe0-4af0-ba9a-a7c44b7dec50",
   "metadata": {},
   "source": [
    "### Set Up Explainer for LIME Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca8086-a2dd-45e0-9117-54557b91c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LIME explainer object with a new name\n",
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(X_train_loaded_scaled.values, \n",
    "                                                        feature_names=X_train_loaded.columns.tolist(), \n",
    "                                                        class_names=['Non Fraud', 'Fraud'], \n",
    "                                                        mode='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4d329-f808-4baf-b4e9-26b73cf69858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the prediction function to ensure it provides probabilities for both classes\n",
    "# Jan 6th - use new loaded model and data\n",
    "def predict_fn(data):\n",
    "    prob_pos = loaded_model.predict(data)\n",
    "    prob_neg = 1 - prob_pos\n",
    "    return np.hstack([prob_neg, prob_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a0467-fa0e-4567-a045-5af58a9c237d",
   "metadata": {},
   "source": [
    "### Sample Feature LIME Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87652523-8824-40b2-b401-fb3188b41015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_explanation(instance, real_value):\n",
    "    \"\"\"\n",
    "    Generate LIME explanation for a specific instance and provide a detailed narrative.\n",
    "    \n",
    "    Parameters:\n",
    "    - instance (numpy array): The instance for which the LIME explanation is to be generated.\n",
    "    - real_value (int): The true label of the instance.\n",
    "    \n",
    "    Returns:\n",
    "    - exp (Lime explanation object): The LIME explanation for the instance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the prediction for the instance\n",
    "    prediction_probs = predict_fn(instance.reshape(1, -1))\n",
    "    \n",
    "    # Generate LIME explanation for the instance using the modified prediction function  \n",
    "    # Jan 6th - use new loaded model and data\n",
    "    #exp = lime_explainer.explain_instance(instance, predict_fn, num_features=len(X_train_loaded.columns))\n",
    "    exp = lime_explainer.explain_instance(instance, predict_fn, num_features=15)\n",
    "    \n",
    "    # Display the LIME explanation in a readable format\n",
    "    print(f'\\n\\n----------------------------------------')\n",
    "    print(f'Real Value: {real_value}')\n",
    "    print(f'Predicted Probability (Fraud): {prediction_probs[0][1]:.2f}\\n')\n",
    "    print('Features in Order of Importance:')\n",
    "    for feature, weight in sorted(exp.as_list(), key=lambda x: abs(x[1]), reverse=True):\n",
    "        feature_parts = feature.split(' ')\n",
    "        feature_name = feature_parts[0]\n",
    "        feature_value = ' '.join(feature_parts[1:])\n",
    "        print(f'Feature: {feature_name} | Value: {feature_value} | Weight: {weight:.2f}')\n",
    "    print('----------------------------------------\\n\\n')\n",
    "    \n",
    "    return exp\n",
    "\n",
    "# The function is now ready to be used. When called, it will provide a detailed explanation for a given instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc292d-95bf-43aa-b071-f78bd7ec2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_explanation_reverseScaled(instance, real_value, scale_loaded, feature_names):\n",
    "    \"\"\"\n",
    "    Generate LIME explanation for a specific instance and provide a detailed narrative.\n",
    "    \n",
    "    Parameters:\n",
    "    - instance (numpy array): The instance for which the LIME explanation is to be generated.\n",
    "    - real_value (int): The true label of the instance.\n",
    "    - scale_loaded (StandardScaler object): The scaler used for inverse transforming the features.\n",
    "    \n",
    "    Returns:\n",
    "    - exp (Lime explanation object): The LIME explanation for the instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert feature_names to a list if it's a Pandas Index\n",
    "    if isinstance(feature_names, pd.Index):\n",
    "        feature_names = feature_names.tolist()\n",
    "        \n",
    "        \n",
    "\n",
    "    # Get the prediction for the instance\n",
    "    prediction_probs = predict_fn(instance.reshape(1, -1))\n",
    "    \n",
    "    # Generate LIME explanation for the instance using the modified prediction function\n",
    "    exp = lime_explainer.explain_instance(instance, predict_fn, num_features=7)\n",
    "    \n",
    "    # Inverse transform the scaled instance\n",
    "    original_instance = scale_loaded.inverse_transform(instance.reshape(1, -1)).flatten()\n",
    "    \n",
    "    # Display the LIME explanation in a readable format\n",
    "    print(f'\\n\\n----------------------------------------')\n",
    "    print(f'Real Value: {real_value}')\n",
    "    print(f'Predicted Probability (Fraud): {prediction_probs[0][1]:.2f}\\n')\n",
    "    print('Features in Order of Importance:')\n",
    "    for feature, weight in sorted(exp.as_list(), key=lambda x: abs(x[1]), reverse=True):\n",
    "        feature_parts = feature.split(' ')\n",
    "        feature_name = feature_parts[0]\n",
    "        # Check if feature_name is non-numeric before using it to look up the index\n",
    "        if is_non_numeric(feature_name):\n",
    "            feature_index = feature_names.index(feature_name)\n",
    "            original_feature_value = original_instance[feature_index]\n",
    "            print(f'Feature: {feature_name} | Value: {original_feature_value} | Weight: {weight:.2f}')\n",
    "        else:\n",
    "            print(f'Feature: {feature} | Weight: {weight:.2f}')\n",
    "    print('----------------------------------------\\n\\n')\n",
    "    \n",
    "    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256dd439-bf3a-4568-96f4-4a5890101ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating LIME explanations for the first three values in the test dataset\n",
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "for i in range(2):\n",
    "    instance = X_test_loaded_scaled.iloc[i].values\n",
    "    real_value = y_test_loaded.iloc[i]\n",
    "    detailed_explanation(instance, real_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72adf2-1aaf-44a4-b7f0-f231991aeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc0968-ed8d-4a28-b978-69f41c543205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_non_numeric(s):\n",
    "    \"\"\"\n",
    "    Check if a string is non-numeric (contains alphanumeric characters).\n",
    "\n",
    "    Parameters:\n",
    "    - s (str): The string to be checked.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the string is non-numeric, False otherwise.\n",
    "    \"\"\"\n",
    "    return not s.replace('.', '', 1).isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a352a-adde-412e-b788-3f51297f0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating LIME explanations for the first three values in the test dataset\n",
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "for i in range(2):\n",
    "    instance = X_test_loaded_scaled.iloc[i].values\n",
    "    real_value = y_test_loaded.iloc[i]\n",
    "    #detailed_explanation_reverseScaled(instance, real_value, scale_loaded, column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17996202-955d-4d55-a7d1-637a675c7385",
   "metadata": {},
   "source": [
    "Narrative:\n",
    "\n",
    "For each instance, the output will display the features in order of their importance in determining the predicted probability.\n",
    "\n",
    "For the instance with a real fraud value of 1:\n",
    "\n",
    "As the predicted probability of a fraudulent transaction is high, this means the model recognizes patterns in this instance similar to other fraudulent instances from the training data.\n",
    "\n",
    "Features with positive weights contributed to increasing the probability of a fraud prediction. The larger the weight, the more influential that feature was.\n",
    "\n",
    "Conversely, features with negative weights worked against the fraud prediction. The larger the negative weight, the more it tried to reduce the probability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For the two instances with a real fraud value of 0:\n",
    "\n",
    "If the predicted probability of a fraudulent transaction is low, this means the model recognizes patterns in this instance similar to other non-fraud instances from the training data.\n",
    "\n",
    "Again, positive weights indicate features that tried to push the prediction towards fraud, while negative weights indicate features that tried to push the prediction away from fraud.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For each feature displayed, the \"Value\" tells you what the specific value of that feature was for the instance, and the \"Weight\" tells you how much that feature influenced the prediction. By examining the top features and their weights, you can get a good understanding of why the model made its prediction.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5f7e0-373b-45b0-ab04-f12204ff4318",
   "metadata": {},
   "source": [
    "### Prediction  Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081f233-4c10-45b6-972c-ce741c7d4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_loaded_scaled\n",
    "# 1. Direct Evaluation\n",
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "sample_instance = X_test_loaded_scaled.iloc[0].values.reshape(1, -1)\n",
    "print(\"Direct model prediction:\", loaded_model.predict(sample_instance))\n",
    "print(\"predict_fn output:\", predict_fn(sample_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a935509-2fd2-4f2e-bf8f-e1789e51b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check Output Shape\n",
    "# Jan 6th - use new loaded model and data\n",
    "assert predict_fn(sample_instance).shape == (1, 2), \"Output shape of predict_fn is incorrect!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3787e-db84-4218-a22b-1efcd1afcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Check Probability Sum\n",
    "# Jan 6th - use new loaded model and data\n",
    "probabilities = predict_fn(sample_instance)\n",
    "assert abs(probabilities[0].sum() - 1) < 1e-6, \"Probabilities do not sum to 1!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24f599-aba2-4f07-9b25-14bdc70c301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_loaded_scaled\n",
    "# 4. Visual Inspection\n",
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "print(\"\\nVisual inspection of predict_fn outputs for first 5 test instances:\")\n",
    "for i in range(5):\n",
    "    instance = X_test_loaded_scaled.iloc[i].values.reshape(1, -1)\n",
    "    print(f\"Instance {i}: {predict_fn(instance)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bceee4-9db7-4e45-a8fa-7f8225e1ebf3",
   "metadata": {},
   "source": [
    "# Prepare LIME Values for Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce2f24d-2977-4396-8886-f2f96588164b",
   "metadata": {},
   "source": [
    "To achieve your goal, we need to:\n",
    "\n",
    "Generate explanations for the chosen instances.\n",
    "Create dataframes for the individual instance features and their corresponding LIME values/weights.\n",
    "Ensure consistent structure for the LIME values/weights dataframe.\n",
    "Save the dataframes to output files.\n",
    "Given the specific structure you need for the LIME values/weights dataframe, we'll also have to ensure that each feature has a consistent number of columns across all instances.\n",
    "\n",
    "Let's proceed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a1850-0d5a-4a11-b1af-5d6de4204cce",
   "metadata": {},
   "source": [
    "We first choose the desired instances, two with a target value of 1 and four with a target value of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0a2d6-5432-40c4-9a2c-800f080d3dd3",
   "metadata": {},
   "source": [
    "The get_detailed_explanation function generates LIME explanations for an instance \n",
    "and returns the feature values and LIME values/weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9324c2a-26c5-4aab-84bf-0e4f7979a8c2",
   "metadata": {},
   "source": [
    "## Generate LIME Details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be23131f-460f-4c03-b3e8-f592d47c2837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get LIME explanations for an instance, function returns explanations\n",
    "def get_detailed_explanation(instance):\n",
    "    \"\"\"\n",
    "    Generate LIME explanation for a specific instance.\n",
    "    \n",
    "    Parameters:\n",
    "    - instance (numpy array): The instance for which the LIME explanation is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "    - feature_values (list): List of feature values for the instance.\n",
    "    - lime_values (list): List of tuples containing LIME values and weights for the instance.\n",
    "    \"\"\"\n",
    "    # Generate LIME explanation for the instance\n",
    "    print(\"Instance type: \", type(instance))\n",
    "    print(\"Instance length: \", len(instance))\n",
    "    \n",
    "    exp = lime_explainer.explain_instance(instance,\n",
    "                                          predict_fn,\n",
    "                                          num_features=32)\n",
    "                                          #num_features=len(X_train_loaded.columns))\n",
    "    \n",
    "    feature_values = instance.tolist()\n",
    "    lime_values = exp.as_list()\n",
    "    \n",
    "    return feature_values, lime_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5ac19-b120-409a-9633-3858597b612a",
   "metadata": {},
   "source": [
    "We then loop over the chosen instances, get their feature values and \n",
    "LIME explanations, and store them in lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1361195-6102-4caa-9d06-ea4d89c70417",
   "metadata": {},
   "source": [
    "## Generate LIME Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2e7c6-7a75-422a-9a4f-6c7e3f8204f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def generate_lime_explanations(df, chosen_indices):\n",
    "    \n",
    "    # Lists to store instance features and LIME values/weights\n",
    "    features_data = []\n",
    "    lime_data = []\n",
    "\n",
    "    # Generate explanations for the chosen instances\n",
    "    for index in chosen_indices:\n",
    "        print(f'Index is {index}')\n",
    "        feature_values, lime_values = get_detailed_explanation(df.loc[index].values)\n",
    "        features_data.append(feature_values)\n",
    "        lime_data.append(lime_values)\n",
    "    \n",
    "    # Jan 6th - use new loaded model and data\n",
    "    # Convert features data to a DataFrame\n",
    "    features_df = pd.DataFrame(features_data, columns=X_train_loaded.columns.tolist())\n",
    "\n",
    "    # Ensure consistent structure for the LIME values/weights dataframe\n",
    "    max_columns = max(len(lime_values) for lime_values in lime_data) * 2  # Multiply by 2 to account for values and weights\n",
    "    lime_array = []\n",
    "\n",
    "    for lime_values in lime_data:\n",
    "        lime_values_array = []\n",
    "        for feature, weight in lime_values:\n",
    "            lime_values_array.extend([feature, weight])\n",
    "        # Padding to ensure consistent number of columns\n",
    "        lime_values_array.extend([None] * (max_columns - len(lime_values_array)))\n",
    "        lime_array.append(lime_values_array)\n",
    "\n",
    "    lime_df = pd.DataFrame(lime_array)\n",
    "    \n",
    "    return features_df, lime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5886f97-a14b-4a24-ada6-86dbd34db903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_feature_names2(feature_descriptors):\n",
    "    \"\"\"\n",
    "    Extracts feature names from a list of LIME explanation descriptors, \n",
    "    accommodating complex feature names including periods and other special characters.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_descriptors (list of str): A list of strings containing LIME explanation descriptors.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list containing the extracted feature names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the regex pattern to match feature names, allowing for periods, underscores, and alphanumeric characters\n",
    "    pattern = r'(?<![\\d<>=])\\b[a-zA-Z_.]+[a-zA-Z0-9_.]*\\b'\n",
    "    \n",
    "    # Initialize an empty list to store the extracted feature names\n",
    "    raw_feature_names = []\n",
    "    \n",
    "    # Iterate over each descriptor in the input list\n",
    "    for descriptor in feature_descriptors:\n",
    "        # Use regex to find all substrings that match the pattern of a feature name\n",
    "        matches = re.findall(pattern, descriptor)\n",
    "        if matches:\n",
    "            # Assuming the first match is the feature name, add it to the list\n",
    "            raw_feature_names.append(matches[0])\n",
    "        else:\n",
    "            # Fallback in case no match is found (shouldn't happen with correct input)\n",
    "            raw_feature_names.append(descriptor)\n",
    "    \n",
    "    # Print the extracted feature names\n",
    "    print(\"Extracted Feature Names:\")\n",
    "    #for name in raw_feature_names:\n",
    "    #    print(name)\n",
    "    \n",
    "    # Return the list of extracted feature names\n",
    "    return raw_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6d923-7355-406a-9318-8ee4dbf58ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "feature_descriptors = [\n",
    "    'CustomerPresentIndicator_N > 1.12', \n",
    "    '-1.16 < TxnChannelCode_OnL <= 0.86', \n",
    "    '>= 2.5 Age', \n",
    "    'Income <= -0.5',\n",
    "    '0.5 < DomesticAuthCount.cnt.hour1 <= 1.0'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99f878-9cc0-4cf6-a531-9ded5e1a372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and get the list of feature names\n",
    "extracted_feature_names = extract_feature_names2(feature_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c5730-3c24-41aa-8cfa-761f897e8492",
   "metadata": {},
   "source": [
    "## Generate LIME Explanations 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65643f6-84cc-4654-9a5d-9fa1af187221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_names(feature_descriptors):\n",
    "    \"\"\"\n",
    "    Extracts feature names from a list of LIME explanation descriptors.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_descriptors (list of str): A list of strings containing LIME explanation descriptors.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list containing the extracted feature names.\n",
    "    \"\"\"\n",
    "    pattern = r'(?<![\\d<>=])\\b[a-zA-Z_.]+[a-zA-Z0-9_.]*\\b'\n",
    "    raw_feature_names = []\n",
    "    for descriptor in feature_descriptors:\n",
    "        matches = re.findall(pattern, descriptor)\n",
    "        if matches:\n",
    "            raw_feature_names.append(matches[0])\n",
    "        else:\n",
    "            raw_feature_names.append(descriptor)\n",
    "    \n",
    "    # Optionally, print the names\n",
    "    #print(\"Extracting Feature Names:\")\n",
    "    #for name in raw_feature_names:\n",
    "    #    print(name)\n",
    "    #print(\"Feature Names Extracted:\")\n",
    "    \n",
    "    return raw_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507553ee-842f-41a3-89c0-9e52267fc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_names_with_default(feature_descriptors, all_feature_names):\n",
    "    \"\"\"\n",
    "    Extracts feature names and aligns them with the original feature names, setting default weight to 0.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_descriptors (list of tuples): LIME explanation descriptors and weights.\n",
    "    - all_feature_names (list of str): All feature names in the order they appear in the instance.\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: A list containing the feature names and weights, aligned with all_feature_names.\n",
    "    \"\"\"\n",
    "    # Create a dictionary for quick lookup of weights by feature name\n",
    "    feature_weights = {re.findall(r'(?<![\\d<>=])\\b[a-zA-Z_.]+[a-zA-Z0-9_.]*\\b', desc)[0]: weight for desc, weight in feature_descriptors}\n",
    "    \n",
    "    # Create structured list with default weight of 0 for features not in explanations\n",
    "    structured_list = [(feature, feature_weights.get(feature, 0)) for feature in all_feature_names]\n",
    "    \n",
    "    return structured_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621f3eb-35b2-4e76-a5fe-25d4733bc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detailed_explanation2(instance):\n",
    "    \"\"\"\n",
    "    Generate LIME explanation for a specific instance and extract feature names.\n",
    "\n",
    "    Parameters:\n",
    "    - instance (numpy array): The instance for which the LIME explanation is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "    - feature_values (list): List of feature values for the instance.\n",
    "    - lime_values (list): List of tuples containing LIME values and weights for the instance.\n",
    "    - feature_names (list): List of feature names for which LIME provided explanations.\n",
    "    \"\"\"\n",
    "\n",
    "    exp = lime_explainer.explain_instance(instance,\n",
    "                                          predict_fn,\n",
    "                                          num_features=16)\n",
    "                                          \n",
    "    feature_values = instance.tolist()\n",
    "    lime_values = exp.as_list()\n",
    "    \n",
    "    # Extract the feature names from the LIME explanations\n",
    "    feature_descriptors = [item[0] for item in lime_values]\n",
    "    feature_names = extract_feature_names(feature_descriptors)\n",
    "    \n",
    "    \n",
    "    # Get all feature names in the order they appear in the instance\n",
    "    all_feature_names = X_train_loaded.columns.tolist()\n",
    "    \n",
    "    # Extract the feature names and weights, and align them with all features, defaulting to 0\n",
    "    lime_values_structured = extract_feature_names_with_default(lime_values, all_feature_names)\n",
    "    \n",
    "    \n",
    "    return feature_values, lime_values, feature_names, lime_values_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629357b6-490c-4d0b-ad59-f924e102d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def generate_lime_explanations2(df, chosen_indices):\n",
    "    \n",
    "    # Lists to store instance features and LIME values/weights\n",
    "    features_data = []\n",
    "    lime_data = []\n",
    "\n",
    "    # Generate explanations for the chosen instances\n",
    "    for index in chosen_indices:\n",
    "        print(f'Index is {index}')\n",
    "        feature_values, lime_values, feature_names, lime_values_structured = get_detailed_explanation2(df.loc[index].values)\n",
    "        features_data.append(feature_values)\n",
    "        lime_data.append(lime_values)\n",
    "    \n",
    "    # Jan 6th - use new loaded model and data\n",
    "    # Convert features data to a DataFrame\n",
    "    features_df = pd.DataFrame(features_data, columns=X_train_loaded.columns.tolist())\n",
    "\n",
    "    # Ensure consistent structure for the LIME values/weights dataframe\n",
    "    max_columns = max(len(lime_values) for lime_values in lime_data) * 2  # Multiply by 2 to account for values and weights\n",
    "    lime_array = []\n",
    "\n",
    "    for lime_values in lime_data:\n",
    "        lime_values_array = []\n",
    "        for feature, weight in lime_values:\n",
    "            lime_values_array.extend([feature, weight])\n",
    "        # Padding to ensure consistent number of columns\n",
    "        lime_values_array.extend([None] * (max_columns - len(lime_values_array)))\n",
    "        lime_array.append(lime_values_array)\n",
    "\n",
    "    lime_df = pd.DataFrame(lime_array)\n",
    "    \n",
    "    return features_df, lime_df, lime_values_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488ea66-a4d4-4b64-ba0a-6255d4756971",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def generate_lime_explanations3(df, chosen_indices):\n",
    "    # Lists to store instance features, LIME values/weights, and structured LIME explanations\n",
    "    features_data = []\n",
    "    lime_data = []\n",
    "    lime_structured_data = []  # New list to store structured LIME data\n",
    "\n",
    "    # Generate explanations for the chosen instances\n",
    "    for index in chosen_indices:\n",
    "        print(f'Index is {index}')\n",
    "        feature_values, lime_values, feature_names, lime_values_structured = get_detailed_explanation2(df.loc[index].values)\n",
    "        features_data.append(feature_values)\n",
    "        lime_data.append(lime_values)\n",
    "        lime_structured_data.append(lime_values_structured)  # Append structured LIME explanations\n",
    "\n",
    "    # Convert features data to a DataFrame - function output 1\n",
    "    features_df = pd.DataFrame(features_data, columns=X_train_loaded.columns.tolist())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Ensure consistent structure for the LIME values/weights dataframe\n",
    "    max_columns = max(len(lime_values) for lime_values in lime_data) * 2  # Multiply by 2 to account for values and weights\n",
    "    lime_array = []\n",
    "\n",
    "    for lime_values in lime_data:\n",
    "        lime_values_array = []\n",
    "        for feature, weight in lime_values:\n",
    "            lime_values_array.extend([feature, weight])\n",
    "        # Padding to ensure consistent number of columns\n",
    "        lime_values_array.extend([None] * (max_columns - len(lime_values_array)))\n",
    "        lime_array.append(lime_values_array)\n",
    "\n",
    "    # Convert feature Lime explanation and weights data to a DataFrame - - function output 2    \n",
    "    lime_df = pd.DataFrame(lime_array)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Create a DataFrame for structured LIME data that alternates feature names and LIME weights\n",
    "    # Prepare column names for the new DataFrame\n",
    "    feature_names = X_train_loaded.columns.tolist()\n",
    "    column_names = []\n",
    "    for i, feature_name in enumerate(feature_names, start=1):\n",
    "        column_names.append(feature_name)  # Feature name\n",
    "        column_names.append(f'Feature_Weight_{i}')  # Corresponding weight column\n",
    "\n",
    "    # Convert structured LIME data into a format suitable for DataFrame construction\n",
    "    lime_structured_array = []\n",
    "    for lime_values_structured in lime_structured_data:\n",
    "        row_data = []\n",
    "        for feature, weight in lime_values_structured:\n",
    "            row_data.extend([feature, weight])\n",
    "        lime_structured_array.append(row_data)\n",
    "\n",
    "    # Create the DataFrame with structured LIME explanations - function output 3\n",
    "    lime_structured_df = pd.DataFrame(lime_structured_array, columns=column_names)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return features_df, lime_df, lime_structured_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f7a7c-3ff0-4951-808a-7eff9036e9c6",
   "metadata": {},
   "source": [
    "The provided Python function; \n",
    "\n",
    "  --  get_detailed_explanation(instance)  --\n",
    "\n",
    "is designed to generate explanations for a specific instance in the credit card fraud dataset using the LIME (Local Interpretable Model-agnostic Explanations) framework. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e491b63-206d-4e35-a478-bf8f15c7f31f",
   "metadata": {},
   "source": [
    "Understanding... \n",
    "\n",
    "  -- exp.as_list() --\n",
    "\n",
    "\n",
    "This method call returns a list of tuples, where each tuple contains two elements:\n",
    "\n",
    " - Feature Name or Descriptor: A string describing a feature (input variable) of the instance being explained. For a credit card fraud dataset, this could be features like transaction amount, transaction time, or various anonymized features often found in such datasets.\n",
    " \n",
    " - LIME Value (Weight or Coefficient): A floating-point number indicating the weight or importance of the feature towards the model's prediction for this specific instance. This value helps to understand how each feature contributes to the model's prediction, whether positively (increasing the likelihood of being classified as fraud) or negatively (decreasing the likelihood of being classified as fraud).\n",
    " \n",
    "The output from exp.as_list() essentially provides a local explanation for why the model made a specific prediction for an instance. Each tuple in the list indicates the direction and magnitude of a feature's impact on the prediction, giving insights into the model's behavior at a fine-grained level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348042c1-1dfd-45f4-93ba-9088401276c4",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf31a0-1247-42ba-804a-499d15a56da3",
   "metadata": {},
   "source": [
    "The feature values are straightforwardly converted to a dataframe.\n",
    "For the LIME values/weights dataframe, we ensure a consistent number of columns \n",
    "across all instances by determining the maximum number of columns needed (max_columns) \n",
    "and then padding each instance's LIME values/weights with None values to match this maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25225202-3d9a-4ead-9533-6eec5e0ac412",
   "metadata": {},
   "source": [
    "# XAI Experiments - Metrics Capture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84f931-b87e-4a95-b551-6c0b90a5503f",
   "metadata": {},
   "source": [
    "## Suppress Warnings to clean up output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57becde9-8f9f-452d-9e54-d70378e9445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74a7704-372d-4703-957a-682ffb40e39d",
   "metadata": {},
   "source": [
    "## Break out Model Test Data into a list of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b03830-c1ee-4402-bb1f-5d452cfca4a7",
   "metadata": {},
   "source": [
    "### Create Test Data for Experiment Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ef029-ae96-43bf-ad79-cde06f533fac",
   "metadata": {},
   "source": [
    "Ensure 'X_test' and 'y_test' Are DataFrames with Proper Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a481a25-f7c7-4654-a8cb-af2d7ea37158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'X_test' is a numpy array and you have a list of the original column names\n",
    "original_feature_names = [col for col in df_downsampled_loaded.columns if col != 'Fraud']\n",
    "\n",
    "# Ensure X_test_loaded has the correct column names (if necessary)\n",
    "X_test_loaded.columns = original_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b37bb-2f76-43bc-8897-bfa911a1c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X_test_loaded and y_test into a single DataFrame\n",
    "df_TestData = pd.concat([X_test_loaded, y_test_loaded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51b7a4-46bf-4c9f-9f3d-7722176f7d0d",
   "metadata": {},
   "source": [
    "### Split the DataFrame into 20 consecutive smaller DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5199d-4b34-4fae-8558-3da1314a8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into 20 consecutive smaller DataFrames\n",
    "split_size, list_df = split_TestData_into_nn_Blocks(df_TestData, num_splits = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c166f2-4bad-4504-bec8-6a2f2876ff99",
   "metadata": {},
   "source": [
    "### Check Label Count for Stability Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c58c50-0697-4ec2-965d-3317eb1d62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrence of each unique value in the 'Fraud' column\n",
    "fraud_counts = df_TestData['Fraud'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Breakdown of 'Fraud' and non-Fraud label records in df_TestData:\")\n",
    "print(fraud_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea3ab2-77dc-43ed-ae24-063576b38417",
   "metadata": {},
   "source": [
    "### Add a routine to check output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f694c0-13dd-4bd6-ad83-2d4b32f66f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display starting points in the first nn sub dataframes\n",
    "startBlockDisplay(df_TestData, split_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a4780-e134-4a5c-ab57-c0b91a3833d0",
   "metadata": {},
   "source": [
    "## Confirm Starting Point in External LIME XAI XL File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c69bd8-186f-403f-99de-742f9968fae7",
   "metadata": {},
   "source": [
    "The code below acts so that for each dataframe in the list just created the following actions are carried out;\n",
    "\n",
    "Check if an XAI results XL spreadsheet called 'LIME_XAI_Metrics_Experiments.xls' exists;\n",
    "\n",
    "If not create an empty XL spreadsheet with the name 'LIME_XAI_Metrics_Experiments.xls', and then define a variable called ‘Sample’ with an integer value of 1 and print the value of 'Sample' to output.\n",
    "\n",
    "If and XL spreadsheet called 'LIME_XAI_Metrics_Experiments.xls' does exist, then read the entries in the spreadsheet in the first column named ‘Sample Number’ and create a variable in this Python program named ‘Sample’ that is one integer value higher than the highest integer number column named ‘Sample Number’ in the XL, and print this value of 'Sample' to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b3bd9-4b6f-491d-87ec-371391608966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential number as an identifier for each DataFrame\n",
    "list_df = {f'df_{i + 1}': list_df[i] for i in range(len(list_df))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a329a-3cde-477d-b04f-8a6bae422515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the LIME XAI metrics results spreadsheet\n",
    "LIME_xai_file_path = 'LIME_XAI_Metrics_Experiments.xlsx'  # Stored locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c1108-d799-4a6c-9066-2b85ffb2459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Function to update or create the spreadsheet and determine the 'Sample' number\n",
    "# Process each dataframe in 'list_df'\n",
    "sample = return_next_sample_number_to_process(list_df, LIME_xai_file_path, \"LIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c648012-837e-4af9-9ab9-c662272ba4bd",
   "metadata": {},
   "source": [
    "## Select Next Dataframe to Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83a251-1b81-45f6-85d0-8cedc2f5b780",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "\n",
    "\t\n",
    "Extend the Python code so that the code reads in the dataframe from 'list df' that corresponds to the integer value in the \n",
    "variable named ‘Sample’. \n",
    "\n",
    "Assign this dataframe the name 'df_Selected_from_List'.\n",
    "\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded4078-f53d-47f5-bf9a-82807cbd53ea",
   "metadata": {},
   "source": [
    "### Initialize Dataframe to Capture Re-start Point as None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae41c1a-7383-4817-a9dd-c6766be81435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize df_Selected_from_List as None\n",
    "df_Selected_from_List = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2140bb3-1b47-4640-90de-5379e1ee9caa",
   "metadata": {},
   "source": [
    "### Extract test data block to restart XAI metrics process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cebe42-3f79-4885-83df-284a7c0c0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Selected_from_List, key = select_restart_testdata_block(df_Selected_from_List, \n",
    "                                                           list_df, \n",
    "                                                           LIME_xai_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b58674-7e6a-41ea-b821-f9d165e84b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no DataFrame is selected (e.g., if 'Sample' exceeds the number of DataFrames in list_df)\n",
    "if 'df_Selected_from_List' not in locals():\n",
    "    print(\"No DataFrame selected. The 'Sample' number may exceed the number of DataFrames in list_df.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0324b26-3243-4ba2-849c-b89a45c0c736",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251e2e4-9a8a-4a1d-abbf-18b52882999f",
   "metadata": {},
   "source": [
    "## Generate XAI Metrics from Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d38ae7-d444-4266-8050-ca226b6d4a5e",
   "metadata": {},
   "source": [
    "### Generate the LIME Values for the Test Data Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc0af54-0375-4872-8680-dca3657e1f48",
   "metadata": {},
   "source": [
    "#### Pre-Process Values for Data Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba61148-f2af-49ce-99a5-9c99bc6e672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Selected_from_List.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1cbc3-6e68-4d2c-8ecb-8de316652c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_Selected_from_List.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5ed2a-61ad-49fe-8994-2244d1edbe36",
   "metadata": {},
   "source": [
    "#### Scale the feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc98e3-6be5-4228-b554-8f91e4b3c789",
   "metadata": {},
   "source": [
    "Call Scaling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb75834-64f0-448c-b35a-bb5a9caff57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_feature_names\n",
    "# Assuming 'X_test' is a numpy array and you have a list of the original column names\n",
    "original_feature_names = [col for col in df_downsampled_loaded.columns if col != 'Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b6597-3090-415b-9170-2199cb0b54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the feature inputs so that they work with the SHAP generation processs\n",
    "df_Selected_Scaled_Data_from_List = scale_feature_inputs(df_Selected_from_List, \n",
    "                                                         original_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c10ce-10a3-4a2d-b93c-fdc6b7e26775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Selected_Scaled_Data_from_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88f59c-2190-41d3-9edd-a37cede13340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_Selected_Scaled_Data_from_List.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31801fb-ec7b-4ca7-b824-54133b8e7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically determine start and end indices\n",
    "start_index = df_Selected_from_List.index[0]  # First index of the DataFrame\n",
    "print(f'Start Index is : {start_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f8fa8-0c4a-4b0b-b6bd-ba9dfebcd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_index = df_Selected_from_List.index[-1]  # Last index of the DataFrame\n",
    "print(f'End Index is : {end_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157057ae-1013-4009-9a04-a2798530b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the range of indices as a list\n",
    "index_range = list(range(df_Selected_Scaled_Data_from_List.index[0],\n",
    "                         df_Selected_Scaled_Data_from_List.index[-1] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb19758-0949-4bc1-893c-4cfc29818780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(index_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a3beae-b1bc-4df8-9758-59e6d7c91e7a",
   "metadata": {},
   "source": [
    "Set limit value (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ab786-dcf7-448e-8ab9-a851ef989718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 'zero' limit value will process the entire data block\n",
    "limit_data_block_rows = 0 # A zero value processes the entire data block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e83302-4659-482f-ab3a-8fc05b00bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 6th - use new loaded model and data\n",
    "default_1_indices = index_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ada27-73ba-431e-b6cb-b398ea24eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging allow a partial subset of data in the data block to be processes\n",
    "if limit_data_block_rows > 0:\n",
    "    chosen_indices = list(np.random.choice(default_1_indices, limit_data_block_rows)) \n",
    "else:\n",
    "    chosen_indices = [i for i in range(65)]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a97ab-4467-4a71-ad42-3b1526e5ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(chosen_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a34bf2-f584-4349-96a0-d2e53b18ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_indices.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc7f5d-d966-4c31-98e1-c20ae1bb20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(chosen_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4899c97f-5720-47bf-8784-1e06d493fbda",
   "metadata": {},
   "source": [
    "Extract the label values from the data block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0b6e1-eba7-41df-a76a-d875a6a388fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_block_labels_df = df_Selected_Scaled_Data_from_List['Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4cca0-a621-4785-98b1-903d9b2db525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_block_labels_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8b006-af50-4a72-b267-b5fc6c3b3a79",
   "metadata": {},
   "source": [
    "#### Get LIME Values for Data Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062ebdb-a279-4969-bd50-2d3db5971361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Selected_from_List = df_Selected_from_List.drop('Fraud', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d136e85-c99f-45a9-8968-3909cba713eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Selected_Scaled_Data_from_List = df_Selected_Scaled_Data_from_List.drop('Fraud', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb406039-8041-4da6-9ebd-75889b76c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feb 12th - fix assignment of weights to features\n",
    "results_LIME, exec_time_LIME = generate_lime_explanations3(df_Selected_Scaled_Data_from_List, \n",
    "                                                          chosen_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154c539-782c-4804-92c3-cb32a6a44e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the results to get features_df, lime_df\n",
    "features_df_LIME, lime_exp_weights_df, lime_values_structured = results_LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7d17d-18fc-4246-8c9e-5d43f2a8b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_df_LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3814b6ce-681d-4500-84fe-d1c18f3fa4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lime_exp_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127c836-3339-4680-b5d3-3a14e2162b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lime_values_structured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be09a8-c781-432b-9809-1538108d6af0",
   "metadata": {},
   "source": [
    "### Remove Feature Names from Feature/Weights Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9227bdc-a197-4b9c-a5f6-2602ed5c8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME Values needs to be limited to numeric values\n",
    "xai_Lime_values_df_numeric = lime_values_structured.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6acae7-f554-4408-b14d-f15872d8f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xai_Lime_values_df_numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3df98-2ba3-4343-8d3b-e3c997930866",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07228dc9-a5d3-4540-a0aa-8bf5428b2495",
   "metadata": {},
   "source": [
    "### Generate Identity Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560652a7-eab7-4dec-85c9-1acab198dfc2",
   "metadata": {},
   "source": [
    "#### Run a Basic Test First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c0a27-7da7-42e7-888d-cc413993164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two random instances from the LIME dataframe\n",
    "random_indices = np.random.choice(xai_Lime_values_df_numeric.index, size=2, replace=False)\n",
    "instance_1 = xai_Lime_values_df_numeric.iloc[random_indices[0]]\n",
    "instance_2 = xai_Lime_values_df_numeric.iloc[random_indices[1]]\n",
    "\n",
    "# Compute the Euclidean distance between the selected instances\n",
    "distance = get_euclidean_distance(instance_1, instance_2)\n",
    "print(f\"Euclidean distance between instance {random_indices[0]} and instance {random_indices[1]}: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2210ffe-cbac-4c75-a05d-1e00f873faac",
   "metadata": {},
   "source": [
    "#### Display LIME Numeric Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5ef50-3489-4c29-ab6a-2d7d96591920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xai_Lime_values_df_numeric.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54338621-6513-4ad7-8c90-d9c5a2c6afe0",
   "metadata": {},
   "source": [
    "#### Retrieve Identity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673fdbc2-6b6f-40e3-adca-5b3fa22c608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(features_df_LIME.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d1867-2552-4e5a-ac75-b898978fdb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(xai_Lime_values_df_numeric.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ceede-23c3-43fe-8eed-f86b482a5047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "LIME_Identity_Metric = get_identity_metric(features_df_LIME, xai_Lime_values_df_numeric, \"LIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468aebc7-c1bb-46d6-83b1-548b3933d9ac",
   "metadata": {},
   "source": [
    "#### Display Identity Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e96dc-cd1c-4950-b71c-905611057e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIME_Identity_Number = \"{:.2f}%\".format(LIME_Identity_Metric)\n",
    "display_text(\"LIME Identity Metric Score: \" + LIME_Identity_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750613a4-116f-4522-af1b-91344543bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in XAI Metric for Identity\n",
    "XAI_Ident_Metric_1 = LIME_Identity_Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b2eab-6a07-4444-87e0-c4bbd8041507",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b81527-151e-4600-a442-d4595c5a5a8a",
   "metadata": {},
   "source": [
    "### Generate Stability Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c98f4-2cda-4f6f-a82f-4ccbf88a2879",
   "metadata": {},
   "source": [
    "#### Pre-Processing of Stability Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c534370-0101-46f4-a859-6f318a698d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_df_LIME.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfe7b9-e95d-476c-8fd0-e20addd1518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xai_Lime_values_df_numeric.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631c5bd-ddb0-4197-83dc-5e90bc584173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356aef8-feb5-4b45-84c1-fbb52a63b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_block_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3fb0b9-1d1f-4f2c-ad9f-d1ec612f16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_block_labels_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29f3a0-861b-47be-b061-5a7608be70eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('y_test_block_labels_df')\n",
    "#print(y_test_block_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d8e58-99f3-40f5-85e4-26f8b50dfe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the label value input to match earlier adjustments in LIME value creations\n",
    "if limit_data_block_rows > 0:\n",
    "    y_test_block_labels_df = y_test_block_labels_df.iloc[:limit_data_block_rows]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8fe3f-8512-4c7c-b0c7-d48728a5ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_block_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db553ea-cb19-4413-949a-a914b357acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the column name 'Fraud'\n",
    "y_test_block_labels_df.columns = ['Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e380bf-176f-4ffc-a379-ffaaf2cc0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_test_block_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a8d91e-80d3-43f0-8b0e-a4703f6cdd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrence of each unique value in the 'Fraud' column\n",
    "fraud_counts_label = df['Fraud'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Breakdown of 'Fraud' and non-Fraud label records in df_TestData:\")\n",
    "print(fraud_counts_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ed4bd-03df-4a63-8bf9-10078501099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the occurrences of each label\n",
    "label_counts = df['Fraud'].value_counts()\n",
    "\n",
    "# Finding the label with the most entries\n",
    "largest_label = label_counts.idxmax()\n",
    "\n",
    "# Assigning it to largest_label_count\n",
    "largest_label_count = label_counts[largest_label]\n",
    "\n",
    "print(\"Label with most entries:\", largest_label)\n",
    "print(\"Count of this label:\", largest_label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22cf54-f036-4589-8575-7434ae5817b3",
   "metadata": {},
   "source": [
    "#### Retrieve Stability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f68c7-dd3b-4966-a601-de6b55b5fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 18th - use new loaded model and data - SCALED\n",
    "LIME_Stability_Metric = get_stability_metric_y(xai_Lime_values_df_numeric, \n",
    "                                               y_test_block_labels_df, \n",
    "                                               largest_label, \n",
    "                                               'LIME')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc147c-3f8b-4e8a-80ba-33cd4eb6b2ea",
   "metadata": {},
   "source": [
    "#### Display Stability Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33f2a7-1146-4299-a96e-6c7a91ccc83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIME_Stability_Number = \"{:.2f}%\".format(LIME_Stability_Metric)\n",
    "display_text(\"SHAP Stability Metric Score: \" + LIME_Stability_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a766f-d41f-4315-a970-79cd39591f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in XAI Metric for Stability\n",
    "XAI_Stability_Metric_2 = LIME_Stability_Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062caccd-d62c-483d-9e79-d46b8af62401",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ad46a-880b-436f-b52b-8868a070877b",
   "metadata": {},
   "source": [
    "### Generate Seperability Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07ef4d-7874-41f8-9505-9f0209cbbc3b",
   "metadata": {},
   "source": [
    "#### Retrieve Seperability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a6ab8-de32-41e2-93a3-81a92db285a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(features_df_LIME.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f764cb-85f7-438d-a807-b33ecf060fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(xai_Lime_values_df_numeric.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a9d3f-2f01-4459-9751-cad3a6e1d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_df_LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3737e03-921d-4404-9269-2cf463cf6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xai_Lime_values_df_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284452b8-b33b-463e-a381-b4af0dbf56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIME_Seperability_Metric = get_seperability_metric(features_df_LIME, \n",
    "                                                   xai_Lime_values_df_numeric, \n",
    "                                                   \"LIME\",\n",
    "                                                   0.80, # threshold  #0.51\n",
    "                                                   0.35) # tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b03cec-e3e7-4f5f-ab24-8bea26d93af6",
   "metadata": {},
   "source": [
    "#### Display Seperability Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7d8ce-17e4-40af-a362-f1223c8fd26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIME_Seperability_Number = \"{:.2f}%\".format(LIME_Seperability_Metric)\n",
    "display_text(\"LIME Seperability Metric Score: \" + LIME_Seperability_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80945e5f-8a09-4638-85ce-242d215a90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in XAI Metric for Seperability\n",
    "XAI_Seperability_Metric_3 = LIME_Seperability_Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2709d914-f539-44f9-8fdd-aee6837f0397",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b9ad7d-656b-4ca8-9c24-9c581511ccec",
   "metadata": {},
   "source": [
    "### Generate Similarity Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2acbb1-6670-4ab0-880e-6c733dc3271d",
   "metadata": {},
   "source": [
    "#### Retrieve Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1846e014-23e3-45a2-9563-baa3e4c93fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(features_df_LIME.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9658deb-d991-499c-8b76-5d35169cf139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(xai_Lime_values_df_numeric.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8608535-3045-403f-913e-4b9ba22c4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIME_Similarity_Metric = get_similarity_metric(features_df_LIME, \n",
    "                                               xai_Lime_values_df_numeric, \n",
    "                                               \"LIME\", \n",
    "                                               use_dbscan=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f190827-ce0f-42e0-b229-2676cb97a09c",
   "metadata": {},
   "source": [
    "#### Display Similarity Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bc5c4-1e13-4318-9b0e-b137dd55f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIME_Similarity_Number = \"{:6.2f}\".format(LIME_Similarity_Metric)\n",
    "display_text(\"LIME Similarity Metric Value: \" + LIME_Similarity_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00089cc-54a1-47b5-a441-a0d0d36d41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in XAI Metric for Similarity\n",
    "XAI_Similarity_Metric_4 = LIME_Similarity_Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c97170-a378-4cc5-97b3-dff685646bc2",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1bca99-aae2-4ce2-972f-891ca12be3d6",
   "metadata": {},
   "source": [
    "### Display Final Set of Metrics (this run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfd572-e7e6-4306-8c41-48804dd8c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(f\"XAI Ident Metric 1: {XAI_Ident_Metric_1}\")\n",
    "print(f\"XAI Stability Metric 2: {XAI_Stability_Metric_2}\")\n",
    "print(f\"XAI Seperability Metric 1: {XAI_Seperability_Metric_3}\")\n",
    "print(f\"XAI Similarity Metric 1: {XAI_Similarity_Metric_4}\")\n",
    "print(f\"XAI Time Metric 5: {exec_time_LIME} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733a791-eee0-4f6a-bff3-a11465353eea",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb60cce-1cc0-4a60-addd-3803a027654c",
   "metadata": {},
   "source": [
    "## Write Out Metrics to XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873866c-3ba5-4075-a777-7b8b70397f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_xai_Metrics_to_XL(LIME_xai_file_path, \n",
    "                        sample, \n",
    "                        LIME_Identity_Metric, \n",
    "                        LIME_Stability_Metric, \n",
    "                        LIME_Seperability_Metric, \n",
    "                        LIME_Similarity_Metric, \n",
    "                        exec_time_LIME, \n",
    "                        df_Selected_from_List,\n",
    "                        \"LIME\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
