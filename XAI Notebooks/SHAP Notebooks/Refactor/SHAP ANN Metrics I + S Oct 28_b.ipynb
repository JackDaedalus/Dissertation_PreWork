{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disseration Experiment 3e\n",
    "# Model Build and SHAP Metric x 2  (Credit Default) October Twenty EightÂ¶\n",
    "Ciaran Finnegan October 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries + Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "import shap\n",
    "import random\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import libraries to build ANN model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Classifier training (not used for explainability)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Import libraries for explainer metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "# Additional display libraires\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./DS_Visualisation_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./XAI_Metrics_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./DS_Model_Evaluation_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualisation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file_to_load = 'credit_default_data.csv'\n",
    "df = pd.read_csv(ds_file_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset to understand its structure\n",
    "styled_dataframe(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset default Pandas display options\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.expand_frame_repr')\n",
    "pd.reset_option('display.max_colwidth')\n",
    "# Display the dataframe\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the target and features to be visualised\n",
    "\n",
    "sTarget_feature = 'default'\n",
    "sFeature_analysis_1 = 'LIMIT_BAL'\n",
    "sFeature_analysis_2 = 'AGE'\n",
    "sFeature_analysis_3 = 'SEX'\n",
    "sFeature3_ticklabel1 = 'Male'\n",
    "sFeature3_ticklabel2 = 'Female'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Visualizations to better understand the data distribution and relationships between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar and Box Plot Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_box_plots(df, sTarget_feature, \n",
    "                       sFeature_analysis_1, \n",
    "                       sFeature_analysis_2, \n",
    "                       sFeature_analysis_3,\n",
    "                       sFeature3_ticklabel1, \n",
    "                       sFeature3_ticklabel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would need feature reduction to work effectively - or some other filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_heatmap(df, \"Credit Default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_distributions(df, \n",
    "                       sFeature_analysis_1, \n",
    "                       sFeature_analysis_2, \n",
    "                       sFeature_analysis_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the threshold for missing values\n",
    "threshold = 0.75 * len(df)\n",
    "\n",
    "# Identify columns with missing values greater than the threshold\n",
    "missing_columns = df.columns[df.isnull().sum() > threshold]\n",
    "\n",
    "# Print the columns with more than 75% missing values\n",
    "print(\"Columns with more than 75% missing values:\", missing_columns)\n",
    "\n",
    "# Drop columns with missing values greater than the threshold\n",
    "df = df.drop(columns=missing_columns)\n",
    "\n",
    "# Save or continue processing with columns removed that had high volumes of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset to re-check structure once any columns with \n",
    "# significant amounts of missing data have been removed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns\n",
    "cat_cols = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "#df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "df_encoded = pd.get_dummies(df, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset to understand its structure\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample Majority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target variable\n",
    "target_distribution = df_encoded['default'].value_counts()\n",
    "\n",
    "target_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the majority and minority classes\n",
    "df_majority = df_encoded[df_encoded['default'] == 0]\n",
    "df_minority = df_encoded[df_encoded['default'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False, \n",
    "                                   n_samples=target_distribution[1], \n",
    "                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the downsampled majority class with the minority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset to mix the data points\n",
    "df_downsampled = df_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the distribution of the target variable in the downsampled dataset\n",
    "df_downsampled['default'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Features + Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "#X = df_encoded.drop('default', axis=1)\n",
    "#y = df_encoded['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the features and target variable\n",
    "X = df_downsampled.drop('default', axis=1)\n",
    "y = df_downsampled['default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Test/Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into inference and training splits\n",
    "#X_train, X_inf, y_train, y_inf = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train_downsampled, X_test_downsampled, y_train_downsampled, y_test_downsampled = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Indexes\n",
    "X_train_downsampled = X_train_downsampled.reset_index(drop=True)\n",
    "X_test_downsampled = X_test_downsampled.reset_index(drop=True)\n",
    "\n",
    "y_train_downsampled = y_train_downsampled.reset_index(drop=True)\n",
    "y_test_downsampled = y_test_downsampled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train into train test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.reset_index(drop=True)\n",
    "#X_test= X_test.reset_index(drop=True)\n",
    "#X_inf = X_inf.reset_index(drop=True)\n",
    "\n",
    "#y_train = y_train.reset_index(drop=True)\n",
    "#y_test= y_test.reset_index(drop=True)\n",
    "#y_inf = y_inf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Additional Data Exploration (Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model Stats\n",
    "print(\"Number of Features:\", X_train_downsampled.shape[1])\n",
    "print(\"Number Continuous Features:\", X_train_downsampled.shape[1] - len(cat_cols))\n",
    "print(\"Number Categorical Features:\", len(cat_cols))\n",
    "print(\"Number Train Examples:\", X_train_downsampled.shape[0])\n",
    "print(\"Number Positive Train Examples:\", (y_train_downsampled == 1).sum())\n",
    "print(\"Number Negative Train Examples:\", (y_train_downsampled == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network (w/TensorFlow/Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with hyperparameter tuning\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(units=hp.Int('units_input', min_value=32, max_value=512, step=32), \n",
    "                    activation='relu', input_shape=(X_train_downsampled.shape[1],)))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_input', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
    "    \n",
    "    for i in range(hp.Int('n_layers', 1, 3)):\n",
    "        model.add(Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dropout_' + str(i), min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,#10\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='helloworld'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning\n",
    "tuner.search(X_train_downsampled, \n",
    "             y_train_downsampled, \n",
    "             epochs=50,#50 \n",
    "             validation_split=0.2, \n",
    "             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units_input')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the best hyperparameters\n",
    "model = build_model(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_downsampled, \n",
    "    y_train_downsampled, \n",
    "    epochs=50, #50\n",
    "    batch_size=32, \n",
    "    validation_split=0.2, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess and Display Model Peformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_funct = display_model_metrics_tabular(model, X_test_downsampled, y_test_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(y_test_downsampled, y_pred_funct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer\n",
    "explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_downsampled, 100)) #100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    # Your code that produces warnings goes here\n",
    "    shap_values = explainer.shap_values(X_test_downsampled.iloc[:100,:], silent=True) #100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_test_downsampled.iloc[:100,:], feature_names=X_train_downsampled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Metric Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shap_explanations(model, data, target_column='default', \n",
    "                                output_instance_file='instances2.csv', \n",
    "                                output_shap_file='shap_values2.csv'):\n",
    "    # Drop the target column from the data\n",
    "    data_features = data.drop(columns=[target_column])\n",
    "    \n",
    "    # Select a subset of the data for explanation (first 25 instances)\n",
    "    instances_to_explain = data_features.iloc[:25, :]#25\n",
    "    \n",
    "    # Create a SHAP explainer\n",
    "    explainer = shap.KernelExplainer(model.predict, shap.sample(data_features, 100)) #100\n",
    "    \n",
    "    # Generate SHAP values for the instances\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        # Your code that produces warnings goes here\n",
    "        shap_values = explainer.shap_values(instances_to_explain)\n",
    "    \n",
    "    # Convert the SHAP values to a DataFrame\n",
    "    if isinstance(shap_values, list):\n",
    "        # For multi-class models, average the SHAP values over all classes\n",
    "        shap_values = np.mean(shap_values, axis=0)\n",
    "    df_shap_values = pd.DataFrame(shap_values, columns=data_features.columns)\n",
    "    \n",
    "    # Output the SHAP values to a csv file\n",
    "    df_shap_values.to_csv(output_shap_file, index=False)\n",
    "    \n",
    "    # Output the instances to a csv file\n",
    "    instances_to_explain.to_csv(output_instance_file, index=False)\n",
    "    \n",
    "    return instances_to_explain, df_shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that `model` is your trained model and `df_encoded` is your dataframe\n",
    "df_instances, df_shap_values = generate_shap_explanations(model, df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Shap Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Random Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random observation from the test dataset\n",
    "random_observation = X_test_downsampled.sample(1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHAP values for the instances\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    # Your code that produces warnings goes here\n",
    "    shap_values_random_observation = explainer.shap_values(random_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the SHAP values for class 1 (default) for this observation\n",
    "shap_values_observation_class1 = shap_values_random_observation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SHAP values to a Series for easier manipulation\n",
    "shap_values_series = pd.Series(shap_values_observation_class1[0], index=random_observation.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the features based on absolute SHAP value\n",
    "sorted_features = shap_values_series.abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top 20 features for the random observation in an aesthetically pleasing tabular format\n",
    "top_20_features_observation = sorted_features.head(20)\n",
    "top_20_features_df_observation = pd.DataFrame({'Feature': top_20_features_observation.index, \n",
    "                                               'SHAP Value': top_20_features_observation.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the index (row number) of the selected observation\n",
    "print(f\"Selected Row Number from Test Data: {random_observation.index[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the SHAP values for the top 20 features of the observation\n",
    "print(\"\\nTop 20 Features and Their SHAP Values:\")\n",
    "display(HTML(xai_styles + top_20_features_df_observation.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate XAI Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a Basic Test First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two random instances from the SHAP value dataframe\n",
    "df_xai_numerical = df_shap_values\n",
    "\n",
    "random_indices = np.random.choice(df_xai_numerical.index, size=2, replace=False)\n",
    "instance_1 = df_xai_numerical.iloc[random_indices[0]]\n",
    "instance_2 = df_xai_numerical.iloc[random_indices[1]]\n",
    "\n",
    "# Compute the Euclidean distance between the selected instances - uses custom project function\n",
    "distance = get_euclidean_distance(instance_1, instance_2)\n",
    "print(f\"Euclidean distance between instance {random_indices[0]} and instance {random_indices[1]}: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve Identity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAP_Identity_Metric = get_identity_metric(df_instances, df_shap_values, \"SHAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_percentage_ident1(features_df, shap_values_df):\n",
    "    \"\"\"\n",
    "    For each instance in the feature dataframe, this function identifies the closest instance \n",
    "    based on Euclidean distance. It then does the same for the corresponding SHAP value. \n",
    "    The function checks if the closest instances for both features and SHAP values match.\n",
    "    \n",
    "    Returns:\n",
    "        Percentage of instances where the closest feature and SHAP value instances match.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize match count to zero\n",
    "    match_count = 0\n",
    "    \n",
    "    # Loop through each instance in the feature dataframe\n",
    "    for idx, instance in features_df.iterrows():\n",
    "        # Compute the Euclidean distance between the current instance and all other instances\n",
    "        feature_distances = features_df.drop(index=idx).apply(lambda row: distance.euclidean(row, instance), axis=1)\n",
    "        \n",
    "        # Identify the index of the closest instance\n",
    "        closest_feature_idx = feature_distances.idxmin()\n",
    "        \n",
    "        # Repeat the process for SHAP values\n",
    "        shap_instance = shap_values_df.loc[idx]\n",
    "        shap_distances = shap_values_df.drop(index=idx).apply(lambda row: distance.euclidean(row, shap_instance), axis=1)\n",
    "        closest_shap_idx = shap_distances.idxmin()\n",
    "        \n",
    "        # Check if the closest instances for both features and SHAP values match\n",
    "        if closest_feature_idx == closest_shap_idx:\n",
    "            match_count += 1\n",
    "        \n",
    "        # Print the distances for debugging purposes\n",
    "        print(f\"Instance {idx}:   Current matches: {match_count}\")\n",
    "        print(f\"\\tClosest feature instance: {closest_feature_idx} (Distance: {feature_distances[closest_feature_idx]:.4f})\")\n",
    "        print(f\"\\tClosest SHAP instance: {closest_shap_idx} (Distance: {shap_distances[closest_shap_idx]:.4f})\")\n",
    "\n",
    "    # Compute the matching percentage\n",
    "    percentage = (match_count / len(features_df)) * 100\n",
    "    print(f\"\\nPercentage of matches: {percentage:.2f}%   {match_count} Matches of {len(features_df)} Entries\")\n",
    "    \n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "match_percentage_ident1(df_instances, df_shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Identity Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAP_Identity_Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAP_Identity_Number = \"{:.2f}%\".format(SHAP_Identity_Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text(\"SHAP Identity Metric Score: \" + SHAP_Identity_Number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Stability Metric Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve Stability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAP_Stability_Metric = get_stability_metric_y(df_shap_values, y_test_downsampled, 'SHAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Stability Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAP_Stability_Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAP_Stability_Number = \"{:.2f}%\".format(SHAP_Stability_Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text(\"SHAP Stability Metric Score: \" + SHAP_Stability_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stability_csv(shap_values_df):\n",
    "    \"\"\"\n",
    "    This function performs the following steps:\n",
    "    1. Clusters the SHAP values into two clusters using the k-means algorithm.\n",
    "    2. Assigns the actual target value from the test dataset to each instance in the SHAP values dataframe.\n",
    "    3. Calculates the percentage of rows where the target class '0' matches the cluster value '0'.\n",
    "    4. Outputs the final dataframe with cluster assignments and actual target values to a CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        Percentage of instances where target class '0' matches cluster value '0'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cluster the SHAP values into two clusters\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42).fit(shap_values_df)\n",
    "    \n",
    "    # Get the cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "    \n",
    "    # Create a new dataframe with an additional column indicating the cluster assignment\n",
    "    clustered_df = shap_values_df.copy()\n",
    "    clustered_df['Cluster'] = cluster_labels\n",
    "    \n",
    "    # Rename clusters so that the largest cluster is always labeled '0'\n",
    "    if sum(cluster_labels) > len(cluster_labels) / 2:\n",
    "        clustered_df['Cluster'] = clustered_df['Cluster'].map({0: '1', 1: '0'})\n",
    "    \n",
    "    # Print the number of instances assigned to each cluster\n",
    "    cluster_0_count = clustered_df[clustered_df['Cluster'] == '0'].shape[0]\n",
    "    cluster_1_count = clustered_df[clustered_df['Cluster'] == '1'].shape[0]\n",
    "    print(f\"Number of Instances in Cluster '0': {cluster_0_count}\")\n",
    "    print(f\"Number of Instances in Cluster '1': {cluster_1_count}\")\n",
    "    \n",
    "    # Assign the appropriate subset of y_test values to the dataframe based on the selected indices\n",
    "    clustered_df['Actual'] = y_test_downsampled.loc[clustered_df.index].values\n",
    "    \n",
    "    # Calculate the percentage of rows where the target class '0' matches the cluster value '0'\n",
    "    matches_0 = clustered_df[(clustered_df['Cluster'] == '0') & (clustered_df['Actual'] == 0)].shape[0]\n",
    "    total_class_0 = clustered_df[clustered_df['Actual'] == 0].shape[0]\n",
    "    \n",
    "    # Calculate the percentage of rows where the target class '1' matches the cluster value '1'\n",
    "    matches_1 = clustered_df[(clustered_df['Cluster'] == '1') & (clustered_df['Actual'] == 1)].shape[0]\n",
    "    total_class_1 = clustered_df[clustered_df['Actual'] == 1].shape[0]\n",
    "    \n",
    "    # Print the results for class '0'\n",
    "    print(f\"\\nFor Class '0':\")\n",
    "    print(f\"Total Instances: {total_class_0}\")\n",
    "    print(f\"Matching Cluster '0' Instances: {matches_0}\")\n",
    "    \n",
    "    # Print the results for class '1'\n",
    "    print(f\"\\nFor Class '1':\")\n",
    "    print(f\"Total Instances: {total_class_1}\")\n",
    "    print(f\"Matching Cluster '1' Instances: {matches_1}\")\n",
    "    \n",
    "    # Output the final dataframe to a CSV file\n",
    "    clustered_df.to_csv('clustered_stability.csv', index=True)\n",
    "    print(\"\\nOutput saved to 'clustered_stability.csv'\")\n",
    "    \n",
    "    return (matches_0 / total_class_0) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "calc_stability_csv(df_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
