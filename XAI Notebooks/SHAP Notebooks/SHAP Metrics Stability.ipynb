{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disseration Experiment 3c\n",
    "# Model Build and SHAP Metric 2  (Credti Default) October ThreeÂ¶\n",
    "Ciaran Finnegan October 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "import shap\n",
    "import random\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Classifier training (not used for explainability)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualisation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file_to_load = 'credit_default_data.csv'\n",
    "df = pd.read_csv(ds_file_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def styled_dataframe(df):\n",
    "    styles = {\n",
    "        'selector': 'table',\n",
    "        'props': [('background-color', '#f4f4f4'),\n",
    "                  ('color', '#000000'),\n",
    "                  ('border-color', '#e0e0e0'),\n",
    "                  ('border', '1px solid #e0e0e0'),\n",
    "                  ('font-family', 'Arial, sans-serif'),\n",
    "                  ('width', '100%')]\n",
    "    }\n",
    "    \n",
    "    # Apply the styles to the dataframe\n",
    "    styled_df = (df.style.set_table_styles([styles])\n",
    "                 .set_properties(subset=df.columns, **{'min-width': '5000px', 'text-align': 'center'})\n",
    "                 .format(None, na_rep='NA'))\n",
    "    \n",
    "    # Convert styled dataframe to HTML and wrap in a div container for scrolling\n",
    "    styled_html = f'<div style=\"width:100%; overflow-x:auto;\">{styled_df.render()}</div>'\n",
    "    \n",
    "    return display(HTML(styled_html))\n",
    "\n",
    "# To check the function (using a sample dataframe)\n",
    "sample_df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [5, 6, 7, 8, 9],\n",
    "    'C': [1, 2, 3, 4, 5],\n",
    "    'D': [5, 6, 7, 8, 9],\n",
    "    'E': [1, 2, 3, 4, 5]\n",
    "})\n",
    "styled_dataframe(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset to understand its structure\n",
    "#df.head()\n",
    "#print(df.head().to_string())\n",
    "styled_dataframe(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset default Pandas display options\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.expand_frame_repr')\n",
    "pd.reset_option('display.max_colwidth')\n",
    "# Display the dataframe\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe as plain text to bypass any CSS/HTML styles\n",
    "print(df.head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the target and features to be visualised\n",
    "\n",
    "sTarget_feature = 'default'\n",
    "sFeature_analysis_1 = 'LIMIT_BAL'\n",
    "sFeature_analysis_2 = 'AGE'\n",
    "sFeature_analysis_3 = 'SEX'\n",
    "sFeature3_ticklabel1 = 'Male'\n",
    "sFeature3_ticklabel2 = 'Female'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Visualizations to better understand the data distribution and relationships between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar and Box Plot Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and axes\n",
    "fig, ax = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot distribution of the dataset target variable\n",
    "sns.countplot(data=df, x=sTarget_feature, ax=ax[0, 0])\n",
    "sPlot_title1 = 'Distribution of ' + sTarget_feature.upper() + ' Status'\n",
    "ax[0, 0].set_title(sPlot_title1)\n",
    "ax[0, 0].set_xticklabels(['Non ' + sTarget_feature.upper(), sTarget_feature.upper()])\n",
    "\n",
    "# Plot distribution of <feature one> based on target variable status\n",
    "sns.boxplot(data=df, x=sTarget_feature, y=sFeature_analysis_1, ax=ax[0, 1])\n",
    "sPlot_title2 = 'Credit Limit Distribution by ' + sTarget_feature.upper() + ' Status'\n",
    "ax[0, 1].set_title(sPlot_title2)\n",
    "ax[0, 1].set_xticklabels(['Non ' + sTarget_feature.upper(), sTarget_feature.upper()])\n",
    "\n",
    "# Plot distribution of <feature two>  based on target variable status\n",
    "sns.boxplot(data=df, x=sTarget_feature, y=sFeature_analysis_2, ax=ax[1, 0])\n",
    "sPlot_title3 = 'Age Distribution by ' + sTarget_feature.upper() + ' Status'\n",
    "ax[1, 0].set_title(sPlot_title3)\n",
    "ax[1, 0].set_xticklabels(['Non ' + sTarget_feature.upper(), sTarget_feature.upper()])\n",
    "\n",
    "# Plot distribution of <feature three> based on target variable status\n",
    "sns.countplot(data=df, x=sFeature_analysis_3, hue=sTarget_feature, ax=ax[1, 1])\n",
    "sPlot_title4 = sFeature_analysis_3.upper() + ' Distribution by ' + sTarget_feature.upper() + ' Status'\n",
    "ax[1, 1].set_title(sPlot_title4)\n",
    "ax[1, 1].set_xticklabels([sFeature3_ticklabel1, sFeature3_ticklabel2])\n",
    "ax[1, 1].legend(title=sTarget_feature.upper() + ' Status', labels=['Non ' + sTarget_feature.upper(), sTarget_feature.upper()])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would need feature reduction to work effectively - or some other filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlation heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distributions for continuous features\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "sns.histplot(df[sFeature_analysis_1], bins=30, ax=ax[0], color=\"skyblue\")\n",
    "ax[0].set_title(\"Distribution of \"+sFeature_analysis_1.upper())\n",
    "ax[0].set_xlabel(sFeature_analysis_1.upper())\n",
    "#ax[0].set_xlabel(\"Credit Limit\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "sns.histplot(df[sFeature_analysis_2], bins=30, ax=ax[1], color=\"salmon\")\n",
    "ax[1].set_title(\"Distribution of \"+ sFeature_analysis_2.upper())\n",
    "ax[1].set_xlabel(sFeature_analysis_2.upper())\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting distributions for categorical features\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.countplot(data=df, x=sFeature_analysis_3, ax=ax[0], palette=\"pastel\")\n",
    "ax[0].set_title(\"Distribution of \" + sFeature_analysis_3.upper())\n",
    "ax[0].set_xlabel(\"Gender (1 = Male, 2 = Female)\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "sns.countplot(data=df, x=\"EDUCATION\", ax=ax[1], palette=\"pastel\")\n",
    "ax[1].set_title(\"Distribution of Education\")\n",
    "ax[1].set_xlabel(\"Education Level\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "sns.countplot(data=df, x=\"MARRIAGE\", ax=ax[2], palette=\"pastel\")\n",
    "ax[2].set_title(\"Distribution of Marital Status\")\n",
    "ax[2].set_xlabel(\"Marital Status\")\n",
    "ax[2].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the threshold for missing values\n",
    "threshold = 0.75 * len(df)\n",
    "\n",
    "# Identify columns with missing values greater than the threshold\n",
    "missing_columns = df.columns[df.isnull().sum() > threshold]\n",
    "\n",
    "# Print the columns with more than 75% missing values\n",
    "print(\"Columns with more than 75% missing values:\", missing_columns)\n",
    "\n",
    "# Drop columns with missing values greater than the threshold\n",
    "df = df.drop(columns=missing_columns)\n",
    "\n",
    "# Save or continue processing with columns removed that had high volumes of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset to re-check structure once any columns with \n",
    "# significant amounts of missing data have been removed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns\n",
    "cat_cols = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust cat columns to range from 0\n",
    "# df[cat_cols] = df[cat_cols] = df[cat_cols].apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "#df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "df_encoded = pd.get_dummies(df, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset to understand its structure\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Features + Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "X = df_encoded.drop('default', axis=1)\n",
    "y = df_encoded['default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Test/Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into inference and training splits\n",
    "X_train, X_inf, y_train, y_inf = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train into train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test= X_test.reset_index(drop=True)\n",
    "X_inf = X_inf.reset_index(drop=True)\n",
    "\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test= y_test.reset_index(drop=True)\n",
    "y_inf = y_inf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Additional Data Exploration (Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model Stats\n",
    "\n",
    "print(\"Number of Features:\", X_train.shape[1])\n",
    "print(\"Number Continuous Features:\", X_train.shape[1] - len(cat_cols))\n",
    "print(\"Number Categorical Features:\", len(cat_cols))\n",
    "print(\"Number Train Examples:\", X_train.shape[0])\n",
    "print(\"Number Positive Train Examples:\", (y_train == 1).sum())\n",
    "print(\"Number Negative Train Examples:\", (y_train == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample Majority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the majority and minority classes in the training data\n",
    "X_train_majority = X_train[y_train == 0]\n",
    "X_train_minority = X_train[y_train == 1]\n",
    "y_train_majority = y_train[y_train == 0]\n",
    "y_train_minority = y_train[y_train == 1]\n",
    "\n",
    "# Under-sample the majority class\n",
    "X_train_majority_downsampled, y_train_majority_downsampled = resample(\n",
    "    X_train_majority, \n",
    "    y_train_majority,\n",
    "    replace=False, \n",
    "    n_samples=len(y_train_minority), \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine the down-sampled majority class with the minority class\n",
    "X_train_downsampled = pd.concat([X_train_majority_downsampled, X_train_minority])\n",
    "y_train_downsampled = pd.concat([y_train_majority_downsampled, y_train_minority])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply RF Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2,\n",
    "                           scoring='recall_macro')\n",
    "\n",
    "grid_search.fit(X_train_downsampled, y_train_downsampled)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Random Forest model\n",
    "# Train the Random Forest classifier with the best parameters\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "#rf_classifier = RandomForestClassifier(random_state=42)\n",
    "#rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LGBMClassifier model\n",
    "lgbm_model = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign model\n",
    "model = rf_classifier \n",
    "#model = lgbm_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train chosen model\n",
    "#model.fit(X_train, y_train)\n",
    "# Retrain the Random Forest classifier on the downsampled data\n",
    "model.fit(X_train_downsampled, y_train_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Model Peformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#accuracy, classification_rep, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics directly from the classification_report function in a structured format\n",
    "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Organize the metrics into a dataframe\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'ROC AUC Score', 'Precision (Class 0)', 'Recall (Class 0)', 'F1-Score (Class 0)', \n",
    "               'Precision (Class 1)', 'Recall (Class 1)', 'F1-Score (Class 1)'],\n",
    "    'Value': [accuracy, roc_auc, \n",
    "              report_dict['0']['precision'], report_dict['0']['recall'], report_dict['0']['f1-score'],\n",
    "              report_dict['1']['precision'], report_dict['1']['recall'], report_dict['1']['f1-score']]\n",
    "})\n",
    "\n",
    "# Display the dataframe in a tabular format\n",
    "display(HTML(metrics_df.to_html(index=False, classes=\"table table-striped table-bordered\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Shap Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SHAP explainer with the trained model\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Compute SHAP values for the test dataset\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Get the SHAP values for class 1 (default)\n",
    "shap_values_class1 = shap_values[1]\n",
    "\n",
    "# Compute the mean absolute SHAP value for each feature\n",
    "mean_shap_values = pd.Series(np.abs(shap_values_class1).mean(axis=0), index=X_test.columns)\n",
    "\n",
    "# Get the top 20 features based on mean absolute SHAP value\n",
    "top_20_features = mean_shap_values.sort_values(ascending=False).head(20)\n",
    "\n",
    "# Display the top 20 features in an aesthetically pleasing tabular format\n",
    "top_20_features_df = pd.DataFrame({'Feature': top_20_features.index, 'SHAP Value': top_20_features.values})\n",
    "\n",
    "# Enhanced table styling using HTML and CSS\n",
    "styles = \"\"\"\n",
    "    <style>\n",
    "        table {\n",
    "            border-collapse: collapse;\n",
    "            width: 50%;\n",
    "            font-family: Arial, sans-serif;\n",
    "        }\n",
    "        th {\n",
    "            background-color: #4CAF50;\n",
    "            color: white;\n",
    "        }\n",
    "        th, td {\n",
    "            border: 1px solid #ddd;\n",
    "            padding: 8px;\n",
    "            text-align: left;\n",
    "        }\n",
    "        tr:nth-child(even) {\n",
    "            background-color: #f2f2f2;\n",
    "        }\n",
    "        tr:hover {\n",
    "            background-color: #ddd;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(styles + top_20_features_df.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown:\n",
    "\n",
    "Initialization: We use the TreeExplainer from the shap library to initialize an explainer object with \n",
    "our trained Random Forest model.\n",
    "\n",
    "Compute SHAP values: The shap_values method of the explainer is used to compute SHAP values for the test dataset. \n",
    "This will give us a matrix where each row corresponds to a sample in the test set and each column represents a feature. \n",
    "The values indicate how much each feature contributed to the prediction for each sample.\n",
    "\n",
    "Mean SHAP values: We take the absolute SHAP values and compute their mean across all samples for each feature. \n",
    "This gives us an idea of the overall importance of each feature.\n",
    "\n",
    "Top 20 features: We then sort the features based on their mean absolute SHAP value and select the top 20.\n",
    "\n",
    "Display: Finally, we display the top 20 features and their associated SHAP values in an aesthetically pleasing \n",
    "tabular format, aligning the text to the left for better readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Random Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random observation from the test dataset\n",
    "random_observation = X_test.sample(1, random_state=42)\n",
    "\n",
    "# Compute the SHAP values for this observation\n",
    "shap_values_random_observation = explainer.shap_values(random_observation)\n",
    "\n",
    "# Get the SHAP values for class 1 (default) for this observation\n",
    "shap_values_observation_class1 = shap_values_random_observation[1]\n",
    "\n",
    "# Convert SHAP values to a Series for easier manipulation\n",
    "shap_values_series = pd.Series(shap_values_observation_class1[0], index=random_observation.columns)\n",
    "\n",
    "# Sort the features based on absolute SHAP value\n",
    "sorted_features = shap_values_series.abs().sort_values(ascending=False)\n",
    "\n",
    "# Display the top 20 features for the random observation in an aesthetically pleasing tabular format\n",
    "top_20_features_observation = sorted_features.head(20)\n",
    "top_20_features_df_observation = pd.DataFrame({'Feature': top_20_features_observation.index, 'SHAP Value': top_20_features_observation.values})\n",
    "\n",
    "# Enhanced table styling using HTML and CSS\n",
    "styles = \"\"\"\n",
    "    <style>\n",
    "        table {\n",
    "            border-collapse: collapse;\n",
    "            width: 50%;\n",
    "            font-family: Arial, sans-serif;\n",
    "        }\n",
    "        th {\n",
    "            background-color: #4CAF50;\n",
    "            color: white;\n",
    "        }\n",
    "        th, td {\n",
    "            border: 1px solid #ddd;\n",
    "            padding: 8px;\n",
    "            text-align: left;\n",
    "        }\n",
    "        tr:nth-child(even) {\n",
    "            background-color: #f2f2f2;\n",
    "        }\n",
    "        tr:hover {\n",
    "            background-color: #ddd;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\"\n",
    "\n",
    "# Display the index (row number) of the selected observation\n",
    "print(f\"Selected Row Number from Test Data: {random_observation.index[0]}\")\n",
    "\n",
    "# Display the SHAP values for the top 20 features of the observation\n",
    "print(\"\\nTop 20 Features and Their SHAP Values:\")\n",
    "display(HTML(styles + top_20_features_df_observation.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Metric Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Filter out instances of class '1'\n",
    "class_1_indices = X_test[y_test == 1].index\n",
    "\n",
    "# Randomly select 10 instances from class '1'\n",
    "selected_class_1_indices = np.random.choice(class_1_indices, 10, replace=False)\n",
    "\n",
    "# Randomly select 25 instances from the remaining dataset\n",
    "remaining_indices = X_test.index.difference(class_1_indices)\n",
    "selected_remaining_indices = np.random.choice(remaining_indices, 25, replace=False)\n",
    "\n",
    "# Combine the selected indices\n",
    "selected_indices = np.concatenate([selected_class_1_indices, selected_remaining_indices])\n",
    "\n",
    "# Extract the selected instances for features\n",
    "features_df = X_test.loc[selected_indices]\n",
    "\n",
    "# Generate SHAP values for the selected instances\n",
    "explainer = shap.TreeExplainer(model)  # Assuming 'model' is the trained Random Forest model\n",
    "shap_values_selected = explainer.shap_values(features_df)\n",
    "\n",
    "# If binary classification, we only need the second set of SHAP values\n",
    "if len(shap_values_selected) == 2:\n",
    "    shap_values_selected = shap_values_selected[1]\n",
    "\n",
    "# Convert SHAP values to a dataframe\n",
    "shap_values_df = pd.DataFrame(shap_values_selected, columns=['SHAP_' + col for col in features_df.columns], index=features_df.index)\n",
    "\n",
    "# Combine features with SHAP values\n",
    "combined_df = pd.concat([features_df, shap_values_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataframe to 'selected_records.csv'\n",
    "combined_df.to_csv('selected_records.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataframe to 'selected_records.csv'\n",
    "features_df.to_csv('features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataframe to 'selected_records.csv'\n",
    "shap_values_df.to_csv('shap_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_values_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_percentage_ident1(features_df, shap_values_df):\n",
    "    \"\"\"\n",
    "    For each instance in the feature dataframe, this function identifies the closest instance \n",
    "    based on Euclidean distance. It then does the same for the corresponding SHAP value. \n",
    "    The function checks if the closest instances for both features and SHAP values match.\n",
    "    \n",
    "    Returns:\n",
    "        Percentage of instances where the closest feature and SHAP value instances match.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize match count to zero\n",
    "    match_count = 0\n",
    "    \n",
    "    # Loop through each instance in the feature dataframe\n",
    "    for idx, instance in features_df.iterrows():\n",
    "        # Compute the Euclidean distance between the current instance and all other instances\n",
    "        feature_distances = features_df.drop(index=idx).apply(lambda row: distance.euclidean(row, instance), axis=1)\n",
    "        \n",
    "        # Identify the index of the closest instance\n",
    "        closest_feature_idx = feature_distances.idxmin()\n",
    "        \n",
    "        # Repeat the process for SHAP values\n",
    "        shap_instance = shap_values_df.loc[idx]\n",
    "        shap_distances = shap_values_df.drop(index=idx).apply(lambda row: distance.euclidean(row, shap_instance), axis=1)\n",
    "        closest_shap_idx = shap_distances.idxmin()\n",
    "        \n",
    "        # Check if the closest instances for both features and SHAP values match\n",
    "        if closest_feature_idx == closest_shap_idx:\n",
    "            match_count += 1\n",
    "        \n",
    "        # Print the distances for debugging purposes\n",
    "        print(f\"Instance {idx}:   Current matches: {match_count}\")\n",
    "        print(f\"\\tClosest feature instance: {closest_feature_idx} (Distance: {feature_distances[closest_feature_idx]:.4f})\")\n",
    "        print(f\"\\tClosest SHAP instance: {closest_shap_idx} (Distance: {shap_distances[closest_shap_idx]:.4f})\")\n",
    "\n",
    "    # Compute the matching percentage\n",
    "    percentage = (match_count / len(features_df)) * 100\n",
    "    print(f\"\\nPercentage of matches: {percentage:.2f}%   {match_count} Matches of {len(features_df)} Entries\")\n",
    "    \n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "match_percentage_ident1(features_df, shap_values_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stability_csv(shap_values_df):\n",
    "    \"\"\"\n",
    "    This function performs the following steps:\n",
    "    1. Clusters the SHAP values into two clusters using the k-means algorithm.\n",
    "    2. Assigns the actual target value from the test dataset to each instance in the SHAP values dataframe.\n",
    "    3. Calculates the percentage of rows where the target class '0' matches the cluster value '0'.\n",
    "    4. Outputs the final dataframe with cluster assignments and actual target values to a CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        Percentage of instances where target class '0' matches cluster value '0'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cluster the SHAP values into two clusters\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42).fit(shap_values_df)\n",
    "    \n",
    "    # Get the cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "    \n",
    "    # Create a new dataframe with an additional column indicating the cluster assignment\n",
    "    clustered_df = shap_values_df.copy()\n",
    "    clustered_df['Cluster'] = cluster_labels\n",
    "    \n",
    "    # Rename clusters so that the largest cluster is always labeled '0'\n",
    "    if sum(cluster_labels) > len(cluster_labels) / 2:\n",
    "        clustered_df['Cluster'] = clustered_df['Cluster'].map({0: '1', 1: '0'})\n",
    "    \n",
    "    # Print the number of instances assigned to each cluster\n",
    "    cluster_0_count = clustered_df[clustered_df['Cluster'] == '0'].shape[0]\n",
    "    cluster_1_count = clustered_df[clustered_df['Cluster'] == '1'].shape[0]\n",
    "    print(f\"Number of Instances in Cluster '0': {cluster_0_count}\")\n",
    "    print(f\"Number of Instances in Cluster '1': {cluster_1_count}\")\n",
    "    \n",
    "    # Assign the appropriate subset of y_test values to the dataframe based on the selected indices\n",
    "    clustered_df['Actual'] = y_test.loc[clustered_df.index].values\n",
    "    \n",
    "    # Calculate the percentage of rows where the target class '0' matches the cluster value '0'\n",
    "    matches_0 = clustered_df[(clustered_df['Cluster'] == '0') & (clustered_df['Actual'] == 0)].shape[0]\n",
    "    total_class_0 = clustered_df[clustered_df['Actual'] == 0].shape[0]\n",
    "    \n",
    "    # Calculate the percentage of rows where the target class '1' matches the cluster value '1'\n",
    "    matches_1 = clustered_df[(clustered_df['Cluster'] == '1') & (clustered_df['Actual'] == 1)].shape[0]\n",
    "    total_class_1 = clustered_df[clustered_df['Actual'] == 1].shape[0]\n",
    "    \n",
    "    # Print the results for class '0'\n",
    "    print(f\"\\nFor Class '0':\")\n",
    "    print(f\"Total Instances: {total_class_0}\")\n",
    "    print(f\"Matching Cluster '0' Instances: {matches_0}\")\n",
    "    \n",
    "    # Print the results for class '1'\n",
    "    print(f\"\\nFor Class '1':\")\n",
    "    print(f\"Total Instances: {total_class_1}\")\n",
    "    print(f\"Matching Cluster '1' Instances: {matches_1}\")\n",
    "    \n",
    "    # Output the final dataframe to a CSV file\n",
    "    clustered_df.to_csv('clustered_stability.csv', index=True)\n",
    "    print(\"\\nOutput saved to 'clustered_stability.csv'\")\n",
    "    \n",
    "    return (matches_0 / total_class_0) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "calc_stability_csv(shap_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
