{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c660707-788c-4887-a18c-943a076e1820",
   "metadata": {},
   "source": [
    "# Disseration Experiment \n",
    "# Dataset Model Building and Evaluation Functions\n",
    "Ciaran Finnegan October 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df040b3-0beb-4741-bef2-db97641ae5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display libraries\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59db0c6-03c7-43e9-a262-db1b498aebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute additional evaluation metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd2e9fd-1303-4dbd-94d1-088503edcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_banner(text):\n",
    "    banner_html = f\"\"\"\n",
    "    <div style=\"background-color: #4CAF50; padding: 7px; text-align: center; border-radius: 3px;\">\n",
    "        <h2 style=\"color: white;\">{text}</h2>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(banner_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0368714-2e00-451d-828b-6b1be7269306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_text(text):\n",
    "    text_html = f\"\"\"\n",
    "    <div style=\"font-size: 20px; font-weight: bold;\">\n",
    "        {text}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(text_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301935b2-063f-42d1-8b24-800a9eae81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_metrics_tabular(model, X_test, y_test):\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    display_banner(\"This is the Model Accuracy\")\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    display_text(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "    #print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_probs]\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "    \n",
    "    \n",
    "    # Creating a formatted table to display the results\n",
    "    table = \"\"\"\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Metric</th>\n",
    "            <th>Value</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Test Accuracy</td>\n",
    "            <td>{:.4f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Precision</td>\n",
    "            <td>{:.4f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Recall</td>\n",
    "            <td>{:.4f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>F1-Score</td>\n",
    "            <td>{:.4f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>ROC-AUC Score</td>\n",
    "            <td>{:.4f}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    \"\"\".format(test_accuracy, precision, recall, f1, roc_auc)\n",
    "    \n",
    "    # Display the table\n",
    "    #display(HTML(table))\n",
    "        \n",
    "    # Extract metrics directly from the classification_report function in a structured format\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Organize the metrics into a dataframe\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'ROC AUC Score', 'Precision (Class 0)', 'Recall (Class 0)', 'F1-Score (Class 0)', \n",
    "                   'Precision (Class 1)', 'Recall (Class 1)', 'F1-Score (Class 1)'],\n",
    "        'Value': [test_accuracy, roc_auc, \n",
    "                  report_dict['0']['precision'], report_dict['0']['recall'], report_dict['0']['f1-score'],\n",
    "                  report_dict['1']['precision'], report_dict['1']['recall'], report_dict['1']['f1-score']]\n",
    "    })\n",
    "\n",
    "    # Display the dataframe in a tabular format\n",
    "    display_text(\"Model Performance Metrics\")\n",
    "    display(HTML(metrics_df.to_html(index=False, classes=\"table table-striped table-bordered\")))\n",
    "    \n",
    "    print(\"Tablular Done!\")\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b8586b3-1ad6-4399-838c-7ca332add134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(y_test, y_pred):\n",
    "\n",
    "    display_banner(\"Confusion Matrix\")\n",
    "    \n",
    "    # Plotting the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return \"Confusion Matrix!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
