{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c660707-788c-4887-a18c-943a076e1820",
   "metadata": {},
   "source": [
    "# Disseration Experiment \n",
    "# Dataset Model Building and Evaluation Functions\n",
    "Ciaran Finnegan February 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df040b3-0beb-4741-bef2-db97641ae5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display libraries\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59db0c6-03c7-43e9-a262-db1b498aebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute additional evaluation metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f2eca1-1eb6-4351-85bf-45d6c8669574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph enhancements\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd2e9fd-1303-4dbd-94d1-088503edcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_banner(text):\n",
    "    banner_html = f\"\"\"\n",
    "    <div style=\"background-color: #4CAF50; padding: 7px; text-align: center; border-radius: 3px;\">\n",
    "        <h2 style=\"color: white;\">{text}</h2>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(banner_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0368714-2e00-451d-828b-6b1be7269306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_text(text):\n",
    "    text_html = f\"\"\"\n",
    "    <div style=\"font-size: 20px; font-weight: bold;\">\n",
    "        {text}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(text_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301935b2-063f-42d1-8b24-800a9eae81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_metrics_tabular(model, X_test, y_test):\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    display_banner(\"This is the Model Accuracy\")\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    display_text(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_probs]\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "    \n",
    "    \n",
    "    # Creating a formatted table to display the results\n",
    "    table = \"\"\"\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Metric</th>\n",
    "            <th>Value</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Test Accuracy</td>\n",
    "            <td>{:.4f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Precision</td>\n",
    "            <td>{:.4f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Recall</td>\n",
    "            <td>{:.4f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>F1-Score</td>\n",
    "            <td>{:.4f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>ROC-AUC Score</td>\n",
    "            <td>{:.4f}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    \"\"\".format(test_accuracy, precision, recall, f1, roc_auc)\n",
    "        \n",
    "    # Extract metrics directly from the classification_report function in a structured format\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Organize the metrics into a dataframe\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'ROC AUC Score', 'Precision (Class 0)', 'Recall (Class 0)', 'F1-Score (Class 0)', \n",
    "                   'Precision (Class 1)', 'Recall (Class 1)', 'F1-Score (Class 1)'],\n",
    "        'Value': [test_accuracy, roc_auc, \n",
    "                  report_dict['0']['precision'], report_dict['0']['recall'], report_dict['0']['f1-score'],\n",
    "                  report_dict['1']['precision'], report_dict['1']['recall'], report_dict['1']['f1-score']]\n",
    "    })\n",
    "\n",
    "    # Display the dataframe in a tabular format\n",
    "    display_text(\"Model Performance Metrics\")\n",
    "    display(HTML(metrics_df.to_html(index=False, classes=\"table table-striped table-bordered\")))\n",
    "    \n",
    "    print(\"Tablular Done!\")\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8586b3-1ad6-4399-838c-7ca332add134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(y_test, y_pred):\n",
    "\n",
    "    display_banner(\"Confusion Matrix\")\n",
    "    \n",
    "    # Plotting the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return \"Confusion Matrix!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5377daca-98a1-4322-9ae5-b79fce5ab17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_CC_binary_cols_cnt(df):\n",
    "    # Identify categorical columns\n",
    "    categorical_features = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Count the unique values in each categorical column\n",
    "    unique_counts = df[categorical_features].nunique()\n",
    "\n",
    "    # Create a DataFrame for better visualization\n",
    "    visualization_df = pd.DataFrame({\n",
    "        \"Categorical Column\": categorical_features,\n",
    "        \"Unique Values Count\": unique_counts\n",
    "    })\n",
    "\n",
    "    # Use the style property to better present the DataFrame\n",
    "    styled_df = visualization_df.style.set_table_styles(\n",
    "        [{\n",
    "            'selector': 'th',\n",
    "            'props': [('font-size', '12pt'), ('background-color', 'lightblue')]\n",
    "        },\n",
    "        {\n",
    "            'selector': 'td',\n",
    "            'props': [('font-size', '12pt')]\n",
    "        }]\n",
    "    ).set_properties(**{\n",
    "        'text-align': 'left',\n",
    "    }).set_caption(\"Categorical Columns and Unique Value Counts\")\n",
    "\n",
    "    # Display the styled DataFrame\n",
    "    return unique_counts, styled_df, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491caed8-7ddd-4628-921f-59dd6454990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_the_features(X_train_downsampled, X_test_downsampled, df_downsampled, sLabel='default'):\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Visualise before scaling\n",
    "    # ----------------------------------------\n",
    "    display_text(\"Visualise before scaling...\")    \n",
    "    \n",
    "    # Convert the downsampled DataFrame to Pandas DataFrame for visualization\n",
    "    X_train_downsampled_df = pd.DataFrame(X_train_downsampled, columns=df_downsampled.columns)\n",
    "    \n",
    "    # Plotting the distributions before scaling\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=X_train_downsampled_df)\n",
    "    plt.title(\"Feature Distributions Before Scaling\")\n",
    "    plt.xticks(rotation=90)\n",
    "    # Disable scientific notation for y-axis\n",
    "    plt.gca().yaxis.set_major_formatter(ticker.ScalarFormatter(useOffset=False))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Isolate Non-Binary features for scaling\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Identify binary features\n",
    "    binary_features = [col for col in X_train_downsampled if \n",
    "                       X_train_downsampled[col].dropna().isin([0, 1]).all()]\n",
    "    print(\"\\n\\nAll CC Fraud Binary features (NOT to be scaled):\", binary_features)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    # Split dataframes into binary and non-binary features\n",
    "    X_train_binary = X_train_downsampled[binary_features]\n",
    "    X_train_non_binary = X_train_downsampled.drop(columns=binary_features)\n",
    "    \n",
    "    X_test_binary = X_test_downsampled[binary_features]\n",
    "    X_test_non_binary = X_test_downsampled.drop(columns=binary_features)\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Scaling the features\n",
    "    # ----------------------------------------\n",
    "    # Scaling the non-binary features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_non_binary_scaled = scaler.fit_transform(X_train_non_binary)\n",
    "    X_test_non_binary_scaled = scaler.transform(X_test_non_binary)\n",
    "    #\n",
    "    \n",
    "    \n",
    "    # Convert scaled arrays back to dataframes\n",
    "    X_train_non_binary_scaled_df = pd.DataFrame(X_train_non_binary_scaled, \n",
    "                                                index=X_train_non_binary.index, \n",
    "                                                columns=X_train_non_binary.columns)\n",
    "    X_test_non_binary_scaled_df = pd.DataFrame(X_test_non_binary_scaled, \n",
    "                                               index=X_test_non_binary.index, \n",
    "                                               columns=X_test_non_binary.columns)\n",
    "    \n",
    "    # Concatenate the binary and scaled non-binary features\n",
    "    X_train_downsampled_scaled = pd.concat([X_train_binary, X_train_non_binary_scaled_df], axis=1)\n",
    "    X_test_downsampled_scaled = pd.concat([X_test_binary, X_test_non_binary_scaled_df], axis=1)\n",
    "    \n",
    "    \n",
    "    # Ensure the sequence of columns matches the original dataframe (excluding the label)\n",
    "    cols = df_downsampled.drop(columns=[sLabel]).columns\n",
    "    X_train_downsampled_scaled = X_train_downsampled_scaled[cols]\n",
    "    X_test_downsampled_scaled = X_test_downsampled_scaled[cols]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Visualise after scaling\n",
    "    # ----------------------------------------\n",
    "    display_text(\"Visualise after scaling...\")    \n",
    "    \n",
    "    # Extract the feature names, excluding the target variable 'default'\n",
    "    feature_names = df_downsampled.drop(sLabel, axis=1).columns\n",
    "    \n",
    "    # Convert the scaled data back to a DataFrame\n",
    "    X_train_scaled_df = X_train_downsampled_scaled\n",
    "    \n",
    "    # Plotting the distributions after scaling\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=X_train_scaled_df)\n",
    "    plt.title(\"Feature Distributions After Scaling\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    return X_train_downsampled_scaled, X_test_downsampled_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52a9545c-be06-44d1-ac50-e823d8d0d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_the_DiCE_df(df_downsampled, scaler, sLabel='Fraud'):\n",
    "    \n",
    "    # The DiCE explainer is built using the entire dataset and \n",
    "    # is scaled seperately from within that Notebook\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Isolate Non-Binary features for scaling\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Identify binary features\n",
    "    binary_features = [col for col in df_downsampled if \n",
    "                       df_downsampled[col].dropna().isin([0, 1]).all()]\n",
    "    print(\"\\n\\nAll CC Fraud Binary features (NOT to be scaled):\", binary_features)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    # Split dataframes into binary and non-binary features\n",
    "    df_downsampled_binary = df_downsampled[binary_features]\n",
    "    df_downsampled_non_binary = df_downsampled.drop(columns=binary_features)\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Scaling the features\n",
    "    # ----------------------------------------\n",
    "    # Scaling the non-binary features\n",
    "    df_downsampled_non_binary_scaled = scaler.transform(df_downsampled_non_binary)\n",
    "    #\n",
    "    \n",
    "    \n",
    "    # Convert scaled arrays back to dataframes\n",
    "    df_downsampled_non_binary_scaled_df = pd.DataFrame(df_downsampled_non_binary_scaled, \n",
    "                                                       index=df_downsampled_non_binary.index, \n",
    "                                                       columns=df_downsampled_non_binary.columns)\n",
    "\n",
    "    \n",
    "    # Concatenate the binary and scaled non-binary features\n",
    "    df_downsampled_scaled = pd.concat([df_downsampled_binary, df_downsampled_non_binary_scaled_df], axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Ensure the sequence of columns matches the original dataframe (excluding the label)\n",
    "    cols = df_downsampled.columns\n",
    "    df_downsampled_scaled = df_downsampled_scaled[cols]\n",
    "\n",
    "\n",
    "    \n",
    "    return df_downsampled_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61203ae1-44b2-457d-ba13-cb1a23e98702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_CC_train_test_data(X_test, y_test, X_train, y_train, df_downsampled, lCatCols, sLabel='_Fraud'):\n",
    "\n",
    "    # Assuming X_train, X_test, y_train, and y_test are already created by the calling Notebook\n",
    "\n",
    "    # Assuming X_train, X_test, y_train, and y_test are numpy arrays - convert\n",
    "    # Test Data\n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    y_test_df = pd.DataFrame(y_test)\n",
    "    \n",
    "    #Training Data\n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    y_train_df = pd.DataFrame(y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Set up file names\n",
    "    X_test_file = 'X_test' + sLabel + '.csv'\n",
    "    y_test_file = 'y_test' + sLabel + '.csv'\n",
    "    X_train_file = 'X_train' + sLabel + '.csv'\n",
    "    y_train_file = 'y_train' + sLabel + '.csv'\n",
    "    dfdownsmp_file = 'df_downsampled' + sLabel + '.csv'\n",
    "    dfCatCols_file = 'df_CatCols' + sLabel + '.csv'\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save the test data\n",
    "    X_test_df.to_csv(X_test_file, index=False)\n",
    "    y_test_df.to_csv(y_test_file, index=False)\n",
    "    \n",
    "    \n",
    "    # Save the training data\n",
    "    X_train_df.to_csv(X_train_file, index=False)\n",
    "    y_train_df.to_csv(y_train_file, index=False)\n",
    "    \n",
    "    \n",
    "    # Save the sample dataset\n",
    "    df_downsampled.to_csv(dfdownsmp_file, index=False)\n",
    "    \n",
    "    \n",
    "    # Save the list of categorical columns in the dataset\n",
    "    dfCatCols = pd.DataFrame(lCatCols)\n",
    "    dfCatCols.to_csv(dfCatCols_file, index=False)\n",
    "\n",
    "\n",
    "    return \"CC Data stored!\", X_test_df, y_test_df, X_train_df, y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ef8572-0a9e-4cf9-ac35-2f318ffed385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CC_train_test_data(sLabel='_Fraud'):\n",
    "    \n",
    "    #Set up file names\n",
    "    X_test_file = 'X_test' + sLabel + '.csv'\n",
    "    y_test_file = 'y_test' + sLabel + '.csv'\n",
    "    X_train_file = 'X_train' + sLabel + '.csv'\n",
    "    y_train_file = 'y_train' + sLabel + '.csv'\n",
    "    dfdownsmp_file = 'df_downsampled' + sLabel + '.csv'\n",
    "    dfCatCols_file = 'df_CatCols' + sLabel + '.csv'\n",
    "    \n",
    "    \n",
    "    # Load the test data\n",
    "    X_test = pd.read_csv(X_test_file)\n",
    "    y_test = pd.read_csv(y_test_file)\n",
    "    \n",
    "    # Load the training data\n",
    "    X_train = pd.read_csv(X_train_file)\n",
    "    y_train = pd.read_csv(y_train_file)\n",
    "    \n",
    "    # Load the downsamples dataset\n",
    "    df_downsampled = pd.read_csv(dfdownsmp_file)\n",
    "    \n",
    "    # Load the list of categorical columns in the dataset\n",
    "    dfCatCols = pd.read_csv(dfCatCols_file)\n",
    "    \n",
    "    return X_test, y_test, X_train, y_train, df_downsampled, dfCatCols"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
